{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, hinge_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import activations, models, layers, losses, optimizers, metrics, regularizers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "regularizers.L1(0.01)",
   "id": "665a9623ae07e280"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ],
   "id": "31311da8b103570c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ],
   "id": "c2a22f3eb8cf99bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ],
   "id": "8f37233abd9bf9d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int8)"
   ],
   "id": "3eba011f960185ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Perceptron\n",
    "\n",
    "Note: we set max_iter and tol explicitly to avoid warnings about the fact that their default value will change in future versions of Scikit-Learn."
   ],
   "id": "494e9659a212fa63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)",
   "id": "a98215b08f8440a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "per_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "per_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = per_clf.predict(X_test)\n",
    "\n",
    "cv_score = cross_val_score(per_clf, X_test, y_test, cv=3, scoring=\"accuracy\")\n",
    "cv_score"
   ],
   "id": "e8893f186fe31304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sdg_clf = SGDClassifier(max_iter=1000,\n",
    "                        tol=1e-3,\n",
    "                        random_state=42,\n",
    "                        loss=\"perceptron\",\n",
    "                        learning_rate=\"constant\",\n",
    "                        eta0=1.0, penalty=None)\n",
    "\n",
    "sdg_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sdg_clf.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "accuracy"
   ],
   "id": "6955a8755a0251ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1] # slope\n",
    "b = -per_clf.intercept_ / per_clf.coef_[0][1]  # y-intercept\n",
    "\n",
    "axes = [0, 5, 0, 2] # x_min, x_max, y_min, y_max\n",
    "\n",
    "x0, x1 = np.meshgrid( # create a grid of points\n",
    "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()] # combine the grid points into pairs\n",
    "y_predict = per_clf.predict(X_new)    # predict the class for each pair\n",
    "zz = y_predict.reshape(x0.shape)      # reshape the predictions to match the grid shape\n",
    "\n",
    "plt.figure(figsize=(10, 4))           # width, height\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
    "\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "\n",
    "save_fig(\"perceptron_iris_plot\")\n",
    "plt.show()"
   ],
   "id": "4f6dbfd420b27853",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Activation functions",
   "id": "db12d40136a1298d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    # Sigmoid activation function\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    # ReLU activation function\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    # Numerical derivative of function f at point z\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ],
   "id": "b5700b1de6b015db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis((-5, 5, -1.2, 1.2))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis((-5, 5, -0.2, 1.2))\n",
    "\n",
    "save_fig(\"activation_functions_plot\")\n",
    "plt.show()"
   ],
   "id": "c6ac2d4ebf0fa88b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def heaviside(z):\n",
    "    # Heaviside step function\n",
    "    return (z >= 0).astype(z.dtype)\n",
    "\n",
    "def mlp_xor(x1, x2, activation=heaviside):\n",
    "    # MLP to compute XOR of two inputs x1 and x2 using given activation function\n",
    "    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)"
   ],
   "id": "f9e5a562ebf527ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x1s = np.linspace(-0.2, 1.2, 100)\n",
    "x2s = np.linspace(-0.2, 1.2, 100)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "z1 = mlp_xor(x1, x2, activation=heaviside)\n",
    "z2 = mlp_xor(x1, x2, activation=sigmoid)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contourf(x1, x2, z1)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: heaviside\", fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contourf(x1, x2, z2)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: sigmoid\", fontsize=14)\n",
    "plt.grid(True)"
   ],
   "id": "b4f224e3cc4917b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Building an Image Classifier\n",
    "\n",
    "- Let's start by loading the fashion MNIST dataset.\n",
    "- Keras has a number of functions to load popular datasets in keras.datasets.\n",
    "- The dataset is already split for you between a training set and a test set, but it can be useful to split the training set further to have a validation set:"
   ],
   "id": "719fd36004e4eed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ],
   "id": "87ad9f0fc5d6d84d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_full.shape",
   "id": "d55846c25c7ae4d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_full.dtype",
   "id": "b661896743100e91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ],
   "id": "be02fbc08de31ce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You can plot an image using Matplotlib's imshow() function, with a 'binary' color map:\n",
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "c994877814ae43f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The labels are the class IDs (represented as uint8), from 0 to 9:\n",
    "y_train"
   ],
   "id": "db2d4a66c33ca7ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Here are the corresponding class names:\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ],
   "id": "8bca4ca79785603f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "class_names[y_train[0]]",
   "id": "68740ef0cdae3517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's take a look at a sample of the images in the dataset:\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "save_fig('fashion_mnist_plot', tight_layout=False)\n",
    "plt.show()"
   ],
   "id": "452b8c9886058f10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create a DNN model\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(shape=[28, 28]), # input layer\n",
    "    keras.layers.Flatten(),  # input layer\n",
    "    keras.layers.Dense(300, activation=\"relu\"),  # hidden layer 1\n",
    "    keras.layers.Dense(100, activation=\"relu\"),  # hidden layer 2\n",
    "    keras.layers.Dense(10, activation=\"softmax\") # output layer\n",
    "])"
   ],
   "id": "c5a073bce8c4c49f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "523c8b383990dba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hidden1 = model.layers[1]\n",
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ],
   "id": "89cfca5bde925b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", # loss function\n",
    "              optimizer=\"sgd\",                        # optimizer\n",
    "              metrics=[\"accuracy\"])                   # metrics to monitor"
   ],
   "id": "a4cc6a0cdc8996ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    verbose=2)"
   ],
   "id": "8f8d84e35d664bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "history.params",
   "id": "ee72d559700073df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.2, 0.95)\n",
    "save_fig(\"keras_learning_curves_plot\")\n",
    "plt.show()"
   ],
   "id": "e4478e110aa8bc7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.evaluate(X_test, y_test)",
   "id": "e6c8819fe47bc249",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_new = X_test[:3]             # pretend we have 3 new images\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ],
   "id": "680c7de296d8a417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_pred"
   ],
   "id": "c7826077764fcf6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.array(class_names)[y_pred]",
   "id": "9cefc216fd6b6a3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ],
   "id": "8e4bc92cae2b7ba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
    "plt.show()"
   ],
   "id": "5a82ede3f93fe68d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Regression MLP\n",
    "- Let's load, split and scale the California housing dataset (the original one, not the modified one as in chapter 2):"
   ],
   "id": "e092c1f6ead1ba95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ],
   "id": "2dbd0afd769b99eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "id": "f85c7d101a0106e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "shape = X_train.shape[1:]\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(shape=shape),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ],
   "id": "d54b7c31992494c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "id": "2e51796bf8ddfa26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_pred",
   "id": "9ae8d474591e7c58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Functional API\n",
    "Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see paper) connects all or part of the inputs directly to the output layer."
   ],
   "id": "6af523f0807672d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "\n",
    "model.summary()"
   ],
   "id": "12d525dbf78e2b94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ],
   "id": "db05e7bb4e843e5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.35, 1.1)\n",
    "plt.show()"
   ],
   "id": "69da8850087d9b07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# What if you want to send different subsets of input features through the wide or deep paths?\n",
    "# We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7).\n",
    "# Note that 3 features will go through both (features 2, 3 and 4).\n",
    "input_A = layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = layers.Dense(30, activation=activations.relu)(input_B)\n",
    "hidden2 = layers.Dense(30, activation=activations.relu)(hidden1)\n",
    "concat = layers.concatenate([input_A, hidden2])\n",
    "output = layers.Dense(1, name=\"output\")(concat)\n",
    "model = models.Model(inputs=[input_A, input_B], outputs=[output])\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B),\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ],
   "id": "bb6cf1632062c443",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.35, 1.1)\n",
    "plt.show()"
   ],
   "id": "b410db1c0c7f563d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adding an auxiliary output for regularization:\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=[\"mse\", \"mse\"],\n",
    "              loss_weights=[0.9, 0.1],\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "\n",
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.35, 1.1)\n",
    "plt.show()"
   ],
   "id": "5f76046e3235f1dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The subclassing API",
   "id": "76a2a018b1719a76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")\n",
    "\n",
    "model.compile(loss=[\"mse\", \"mse\"],\n",
    "              loss_weights=[0.9, 0.1],\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B),\n",
    "                    (y_train, y_train),\n",
    "                    epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0.52, 5)\n",
    "plt.show()"
   ],
   "id": "658044f3baaa37dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Saving and Restoring",
   "id": "61d5dde7b5aaabe2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ],
   "id": "ea663d97186fa80b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save(\"my_keras_model.keras\")",
   "id": "e786b368ab155f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = keras.models.load_model(\"models/my_keras_model.keras\")",
   "id": "d451b6ec7c7f16ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.predict(X_new)",
   "id": "8fbfeb6af4d92d68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save_weights(\"my_keras_weights.weights.h5\")",
   "id": "7848654dc195f822",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.load_weights(\"my_keras_weights.weights.h5\")",
   "id": "80ebbabbb8d239ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Using Callbacks during Training",
   "id": "6430b5bf6b5ef69e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(shape=[8]),\n",
    "    keras.layers.Dense(32, activation=activations.relu),\n",
    "    keras.layers.Dense(16, activation=activations.relu),\n",
    "    keras.layers.Dense(1, activation=activations.softplus)\n",
    "])\n",
    "\n",
    "model.compile(loss=losses.Huber(),\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-3, epsilon=0.001),\n",
    "              metrics=[metrics.RootMeanSquaredError()])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"models/my_keras_model.keras\", save_best_only=True)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"models/my_keras_model.keras\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ],
   "id": "3096b75e377e0474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display model history metrics by epoch\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "id": "1ad268e173162592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb,      # Save model checkpoints\n",
    "                               early_stopping_cb]) # Stop if no progress for 10 epochs\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ],
   "id": "b01700ac4fad2bcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    \"\"\" Callback to print the ratio of validation loss to training loss at the end of each epoch.\"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\" Called at the end of each epoch. \"\"\"\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "val_train_ratio_cb = PrintValTrainRatioCallback() # create the callback instance\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb]) # use the callback during training"
   ],
   "id": "fdfd5aebe24bba59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Wide & Deep model",
   "id": "60bc0bf98813ad3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "input_ = layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = layers.Dense(30, activation=activations.relu)(input_)  # provide as input to first hidden layer\n",
    "hidden2 = layers.Dense(30, activation=activations.relu)(hidden1) # provide hidden layer 1 as input to hidden layer 2\n",
    "concat = layers.Concatenate()([input_, hidden2])                 # concatenate input and hidden layer 2\n",
    "output = layers.Dense(1, activation=activations.softplus)(concat)                                 # output layer\n",
    "\n",
    "model = models.Model(inputs=[input_], outputs=[output])          # create the model\n",
    "\n",
    "model.compile(loss=losses.Huber(),\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-3,\n",
    "                                        epsilon=0.001),\n",
    "              metrics=[metrics.RootMeanSquaredError()])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    )\n",
    "\n",
    "# display model history metrics by epoch\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "id": "846f8114baad0ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Additional Output for DNN model",
   "id": "f293684796c81037"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "# plot_model(\n",
    "#     model,\n",
    "#     to_file=\"wide_deep_model.png\",\n",
    "#     show_shapes=True,      #     показати розміри тензорів\n",
    "#     show_layer_names=True, # показати назви шарів\n",
    "#     expand_nested=True\n",
    "# )\n",
    "\n",
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\n",
    "\n",
    "# display model history metrics by epoch\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "id": "ddfe6f425e2551e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dynamic Models by Subclassing ( experimental, cannot be saved/cloned/loaded)",
   "id": "c29fd444c18b5455"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ],
   "id": "92a989dbbc8e807",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Callbacks",
   "id": "79ac089fc10e5414"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    \"models/my_keras_model.keras\",\n",
    "    save_best_only=True\n",
    ")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=[\"mse\", \"mse\"],\n",
    "    loss_weights=[0.9, 0.1],\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=1e-3)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    (X_train_A, X_train_B),\n",
    "    (y_train, y_train),\n",
    "    epochs=100,\n",
    "    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)),\n",
    "    callbacks=[checkpoint_callback, # Model Checkpoint ( save best model )\n",
    "               early_stopping_callback]) # Early Stopping ( stop if no progress for 10 epochs )\n",
    "\n",
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    \"\"\" Callback to print the ratio of validation loss to training loss at the end of each epoch.\"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\" Called at the end of each epoch. \"\"\"\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\" Called at the beginning of training. \"\"\"\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\" Called at the end of training. \"\"\"\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\" Called at the beginning of each epoch. \"\"\"\n",
    "        print(f\"Starting epoch {epoch + 1}...\")"
   ],
   "id": "a68473755ca60ed6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### TensorBoard",
   "id": "139b2e67d6d0968c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # example: \"logs/run_2024_06_01-12_00_00\"\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=run_logdir,\n",
    "    histogram_freq=1,\n",
    "    profile_batch=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    (X_train_A, X_train_B),\n",
    "    (y_train, y_train),\n",
    "    epochs=20,\n",
    "    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ],
   "id": "9e933fe12a55cd10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run TensorBoard in Jupyter Notebook",
   "id": "b17a57480c77d45e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs --port=600"
   ],
   "id": "5b50ec4dd85cb93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter Tuning",
   "id": "6047d61360c87150"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using SKLearnRegressor wrapper without RandomSearch",
   "id": "63be4c537a10dbd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.wrappers import SKLearnRegressor\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8], X=None, y=None):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "keras_reg = SKLearnRegressor(build_model)\n",
    "\n",
    "keras_reg.fit(X_train,\n",
    "              y_train,\n",
    "              epochs=100,\n",
    "              verbose=0,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)]\n",
    "              )\n",
    "\n",
    "y_pred = keras_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "y_real = y_test[:10]\n",
    "y_pred_sample = y_pred[:10]\n",
    "\n",
    "print(f'Predicted values: {y_pred_sample}')\n",
    "print(f'Real values: {y_real}')\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MSE Test: {mse}')"
   ],
   "id": "146f55aa2688b940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Using KerasRegressor (scikeras ) with RandomizedSearchCV",
   "id": "984ad303601282a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data\n",
    "housing = fetch_california_housing()\n",
    "X_full, X_test, y_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Model builder\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.InputLayer(shape=input_shape))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# SciKeras wrapper\n",
    "reg = KerasRegressor(\n",
    "    model=build_model,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Hyperparameter distributions (model build args + training args)\n",
    "param_distribs = {\n",
    "    \"model__n_hidden\": [1, 2, 3],\n",
    "    \"model__n_neurons\": [16, 32, 64],\n",
    "    \"model__learning_rate\": reciprocal(1e-4, 1e-2),\n",
    "    \"batch_size\": [32, 64],\n",
    "    \"epochs\": [5, 10, 20]\n",
    "}\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=reg,\n",
    "    param_distributions=param_distribs,\n",
    "    n_iter=5,\n",
    "    cv=3,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"Best params:\", search.best_params_)\n",
    "best_reg = search.best_estimator_\n",
    "y_pred = best_reg.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test RMSE:\", rmse)"
   ],
   "id": "1db3dc6ef80b1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Correct using BatchNormalization",
   "id": "5d8828084766e0b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(shape=[8]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(n_units=30,\n",
    "                       activation='he_normal', # He initialization for ReLU\n",
    "                       use_bias=False), # no bias when using BatchNorm\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(n_units=30,\n",
    "                       activation='he_normal',\n",
    "                       use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ],
   "id": "e89039064377969a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DNN Transfer Learning Example ( make sense on large DNN models and large datasets )",
   "id": "e2d06be3136985f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_A = keras.models.load_model(\"models/my_keras_model_a.keras\") # pre-trained model A\n",
    "model_B_ON_A = keras.models.Sequential(model_A.layers[:-1])        # all layers except the output layer\n",
    "model_B_ON_A.add(keras.layers.Dense(1, activation='sigmoid'))      # new output layer"
   ],
   "id": "342e2b5a46c06b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### But if model_A is changed, model_B_ON_A will also change! To avoid this problem, we can clone model_A:",
   "id": "562be55f3cfedfec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)                  # clone architecture\n",
    "model_A_clone.set_weights(model_A.get_weights())                   # copy weights\n",
    "model_B_ON_A = keras.models.Sequential(model_A_clone.layers[:-1])  # all layers except the output layer\n",
    "model_B_ON_A.add(keras.layers.Dense(1, activation='sigmoid'))      # new output layer\n",
    "\n",
    "# to avoid training all layers from scratch, we can freeze the pre-trained layers:\n",
    "for layer in model_B_ON_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_ON_A.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "# now we can train model by few epochs after this unfreezing all layers and lowering the learning rate\n",
    "history = model_B_ON_A.fit(X_train,\n",
    "                             y_train,\n",
    "                             epochs=5,\n",
    "                             validation_data=(X_valid, y_valid))\n",
    "\n",
    "for layer in model_B_ON_A.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-4) # lower learning rate\n",
    "model_B_ON_A.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=optimizer,\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B_ON_A.fit(X_train,\n",
    "                             y_train,\n",
    "                             epochs=10,\n",
    "                             validation_data=(X_valid, y_valid))"
   ],
   "id": "71ff1b03c854fcca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
