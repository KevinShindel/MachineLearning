
## Vanishing and exploding gradients

> - The delta computed should be of the right size for gradual descent
> - Too small
>   - Decay, no changed to weights 
> - Too big
>   - Choppy learning with no descend 

> **Gradient Descent**
> - The gradient descent algorithm multiplies the gradient by a scalar known as the learning rate to determine the next point.

> **Solutions**
> - Weight Initialization
> - Activation Functions
> - Batch Normalization

## Batch Normalization

> - Normalize the inputs before each hidden layer
> - Center and scale (StandardScaler)
> - Normalizes the inputs to be of the same scale
> - Helps attain higher accuracies with lower epochs
> - Additional computation and increased inference times

```python
accuracy_measures = {}

normalization_list = ['none','batch']
for normalization in normalization_list:
    
    model_config = base_model_config()
    X,Y = get_data()
    
    model_config["NORMALIZATION"] = normalization
    model_name="Normalization-" + normalization
    history=create_and_run_model(model_config,X,Y,model_name)
    
    accuracy_measures[model_name] = history.history["accuracy"]
    
plot_accuracy(accuracy_measures)
```


## Optimizers

> - Regular gradient descent can be slow
> - Takes a lot of time to get closer to the desired accuracy
> - More training time and resources
> - Limited training data may also impact gradient descent
> - Optimizers help speed up the training process
> - Changes the delta value to get closer to desired state

> **Available Optimizers**
> - SGD (Stochastic Gradient Descent)
> - RMSprop (Root Mean Square Propagation)
> - Adam (Adaptive Moment Estimation)
> - Adagrad (Adaptive Gradient Algorithm)

## Optimizer experiment

```python
accuracy_measures = {}

optimizer_list = ['sgd','rmsprop','adam','adagrad']
for optimizer in optimizer_list:
    
    model_config = base_model_config()
    X,Y = get_data()
    
    model_config["OPTIMIZER"] = optimizer
    model_name = "Optimizer-" + optimizer
    history=create_and_run_model(model_config,X,Y, model_name)
    
    accuracy_measures[model_name] = history.history["accuracy"]
```

## Learning rate

> - Rate at witch the weights are change in response to the estimated error
> - Works in conjunction with the optimizer
> - Numeric value used to adjust the delta computed


> **Learning Rate Selection**
> - Large Value:
>   - Faster learning with fewer epochs
>   - Risk of exploding gradients
> - Small Value:
>   - Slower learning with more epochs
>   - Risk of vanishing gradients

## Learning rate experiment

```python
accuracy_measures = {}

learning_rate_list = [0.001, 0.005,0.01,0.1,0.5]
for learning_rate in learning_rate_list:
    
    model_config = base_model_config()
    X,Y = get_data()
    
    model_config["LEARNING_RATE"] = learning_rate
    model_name="Learning-Rate-" + str(learning_rate)
    history=create_and_run_model(model_config,X,Y, model_name)
    
    #accuracy
    accuracy_measures[model_name] = history.history["accuracy"]

plot_graph(accuracy_measures, "Compare Learning Rates")
```


