# Tuning Deep Learning Network model

## Epoch and batch size tuning

**Batch Size** - A set of samples sent trough ANN in a single pass

> **Higher the batch size**
> - Faster training
> - More memory required
> - Better GPU utilization
> - Instability during training progress

> **Recommendation**
> - Experiment with batch size (optimal around 32)


### Epochs

> - The number of times the entire dataset is passed forward and backward through the neural network
> - Controls only training, not inference
> - As epochs increase, gains taper off and may lead to instability

> **Recommendation**
> - Choose the earliest value when accuracy stabilize

## Epoch and batch size experiment

```python
#Initialize the measures
accuracy_measures = {}

for batch_size in range(16,128,16):
    
    #Load default configuration
    model_config = base_model_config()
    #Acquire and process input data
    X,Y = get_data()
    
    #set epoch to 20
    model_config["EPOCHS"]=20
    
    #Set batch size to experiment value
    model_config["BATCH_SIZE"] = batch_size
    model_name = "Batch-Size-" + str(batch_size)
    
    history=create_and_run_model(model_config,X,Y,model_name)
    
    accuracy_measures[model_name] = history.history["accuracy"]
```

> After running the experiment, the following results were obtained:
> - Optimal batch size is 16
> - Optimal epoch is 25

## Hidden layer tuning

> **More layers**
> - Possibility to learn complex relationships
> - More training and inference time and cost
> - Overfitting to training set

> **Recommendations**
> - Two layers found sufficient for most problems
> - Experiment with more layers for complex problems
> - Increase based on experimentation


# Hidden layer experiment

```python
accuracy_measures = {}
layer_list =[]
for layer_count in range(1,6):
    
    #32 nodes in each layer
    layer_list.append(32)
    
    model_config = base_model_config()
    X,Y = get_data()
    
    model_config["HIDDEN_NODES"] = layer_list
    model_name = "Layers-" + str(layer_count)
    history=create_and_run_model(model_config,X,Y,model_name)
    
    accuracy_measures[model_name] = history.history["accuracy"]
```

> After running the experiment, the following results were obtained:
> - Optimal batch size is 16
> - Optimal epoch is 21
> - Optimal number of layers is 4
> - Increasing of layers do not give significant improvement


## Determining nodes in a layer

> **More nodes**
> - Possibility to learn complex relationships
> - More training and inference time and cost 
> - Overfitting to training set


> **Recommendations**
> - Start with 32 nodes
> - Between number of input and output nodes
> - Increase based on experimentation
> - Optimal number of nodes between 32 and 64 but not more 128


# Nodes in a layer experiment

```python
accuracy_measures = {}

for node_count in range(8, 40, 8):

    # have 2 hidden layers in the networks
    layer_list = []
    for layer_count in range(2):
        layer_list.append(node_count)

    model_config = base_model_config()
    X, Y = get_data()

    model_config["HIDDEN_NODES"] = layer_list
    model_name = "Nodes-" + str(node_count)
    history = create_and_run_model(model_config, X, Y, model_name)

    accuracy_measures[model_name] = history.history["accuracy"]

plot_graph(accuracy_measures, "Compare Batch Size and Epoch")
```

> After running the experiment, the following results were obtained:
> - Optimal batch size is 16
> - Optimal epoch is 25
> - Optimal number of layers is 4
> - Optimal number of nodes is 32
> - Decreasing either increasing of nodes do not give significant improvement
> - Best result with adjusted hyperparameters is 0.98

## Choosing activation functions

> **Activation functions**
> - Depends upon the problem and the network chosen
> - May impact gradient descent and learning rate


> **Recommendations**
> ReLU (Rectified Linear Unit) - works best for ANN and CNN 
> Sigmoid - works best for RNN
> Experiment with different activation functions - may be necessary


> **Activation: Output layer**
> - Sigmoid - Binary classification
> - Based on the problem - Softmax, ReLU, Tanh, etc.
> - Softmax - Multi-class classification
> - Regression - Linear activation


## Activation function experiment

```python
accuracy_measures = {}

activation_list = ['relu', 'sigmoid', 'tanh']

for activation in activation_list:
    model_config = base_model_config()
    X, Y = get_data()

    model_config["HIDDEN_ACTIVATION"] = activation
    model_name = "Model-" + activation
    history = create_and_run_model(model_config, X, Y, model_name)

    accuracy_measures["Model-" + activation] = history.history["accuracy"]

plot_graph(accuracy_measures, "Compare Batch Size and Epoch")
```

> After running the experiment, the following results were obtained:
> - Optimal batch size is 16
> - Optimal epoch is 25
> - Optimal number of layers is 4
> - Optimal number of nodes is 32
> - Best activation function is Tanh
> - Best result with adjusted hyperparameters is 0.98

## Initialization of weights

| Initialization | Initialize to                                    |
|----------------|--------------------------------------------------|
| Random         | Random values  from standart normal distribution |
| Zeros          | All weights are initialized to zero              |
| Ones           | All weights are initialized to one               |
| Random Uniform | Random values from uniform distribution          |
| Xavier         | Xavier Glorot initialization                     |
| He             | He initialization                                |

> **Recommendations**
> - Xavier initialization - works best for ReLU
> - He initialization - works best for Tanh
> - Experiment with different initialization methods
> - May be necessary to adjust learning rate
> - Random normal works best for most problems


## Weight initialization experiment

```python
accuracy_measures = {}

initializer_list = ["random_normal",
                    'zeros',
                    'ones',
                    "random_uniform",
                    "he_normal",
                    "he_uniform",
                    "lecun_normal",
                    "lecun_uniform",
                    "glorot_normal",
                    "glorot_uniform"]

for initializer in initializer_list:
    model_config = base_model_config()
    X, Y = get_data()

    model_config["WEIGHTS_INITIALI4=0ZER"] = initializer
    model_name = "Model-" + initializer
    history = create_and_run_model(model_config, X, Y, model_name)

    accuracy_measures[model_name] = history.history["accuracy"]

plot_graph(accuracy_measures, "Compare Batch Size and Epoch")
```

> After running the experiment, the following results were obtained:
> - Optimal batch size is 16
> - Optimal epoch is 25
> - Optimal number of layers is 4
> - Optimal number of nodes is 32
> - Best activation function is Tanh
> - Best weight initialization is He
> - Best result with adjusted hyperparameters is 0.98

