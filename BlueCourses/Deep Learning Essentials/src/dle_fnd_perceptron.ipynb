{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dle_fnd_perceptron.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPu60Jouf+AfI+d8eU5weoT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7GlFtQFp1_GD","colab_type":"text"},"source":["In this notebook, we're going to create a simple perceptron using pure Python (no TensorFlow or Keras yet). To keep things simple, we're not even going to use Numpy here.\n","\n","The only standard library we're going to bring in is `math`, to define the sigmoid activation function."]},{"cell_type":"code","metadata":{"id":"i8wJdjA7tAKZ","colab_type":"code","colab":{}},"source":["import math"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2y-EuB42WAC","colab_type":"text"},"source":["First, we define our data set. `X` defines a set of two-dimensional data points, and `y` defines our vector of outcomes (targets).\n","\n","Note: typically, we'd want to normalize, standardize, or minmax scale our instances first to make the network train better. Given that our samples aren't too extreme here, we can safely skip this step."]},{"cell_type":"code","metadata":{"id":"qJ4W-Hgn2W0a","colab_type":"code","colab":{}},"source":["X = [[0, 1], [1, 0], [2, 2], [3, 4], [4, 2], [5, 2], [4, 1], [5, 0]]\n","y = [0, 0, 0, 0, 1, 1, 1, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCazorAt2XWS","colab_type":"text"},"source":["Next up, we define our activation function, and can immediately define a function to predict an outcome given an instance. Since we use a sigmoid activation function, the output of the perceptron will be bounded between 0 and 1 and can be directly interpreted as a probability."]},{"cell_type":"code","metadata":{"id":"5QDqBO5m67dp","colab_type":"code","colab":{}},"source":["def sigmoid(x):\n","  return 1 / (1 + math.exp(-x))\n","\n","def predict(instance, weights):\n","  # We need a weight for each input, plus a bias weight\n","  assert len(weights) == len(instance) + 1\n","  # Assume that the first weight given is our bias\n","  output = weights[0]\n","  for i in range(len(weights)-1):\n","    output += weights[i+1] * instance[i]\n","  return sigmoid(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DzjZRY-3Qac","colab_type":"text"},"source":["We can now already see what happens if we let an untrained perceptron make some predictions, e.g. by setting all the weights to 0.\n","\n","Setting these initial values for the weights is typically called \"initialization\", and is an important topic on its own we'll discuss later on."]},{"cell_type":"code","metadata":{"id":"qMyECl4_7MXu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":155},"executionInfo":{"status":"ok","timestamp":1595328706115,"user_tz":-120,"elapsed":779,"user":{"displayName":"Seppe vanden Broucke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMpesh_oeuo7Fc2WsgK89OJRIeyn_p5F2LV-ineDc=s64","userId":"09380512084218149317"}},"outputId":"d9fb405e-d5cb-46a7-8ffc-10c2d4ff60ab"},"source":["weights = [0, 0, 0]\n","\n","for i in range(len(X)):\n","  prediction = predict(X[i], weights)\n","  print(X[i], y[i], '->', prediction)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0, 1] 0 -> 0.5\n","[1, 0] 0 -> 0.5\n","[2, 2] 0 -> 0.5\n","[3, 4] 0 -> 0.5\n","[4, 2] 1 -> 0.5\n","[5, 2] 1 -> 0.5\n","[4, 1] 1 -> 0.5\n","[5, 0] 1 -> 0.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1V8_g8SH3jhc","colab_type":"text"},"source":["Next, we define a method to train our perceptron using one instance. This is done by calculating the error or \"loss\", which is then used to shuffle the weights around.\n","\n","To do so, we need to calculate the gradient of our loss function with respect to the weights: $\\frac{\\partial L}{\\partial w}$.\n","\n","Let us define our loss function as $L = (y - \\hat{y})^2$. Remember that our prediction is given by $\\sigma(o)$ with $\\sigma$ the sigmoid function and $o = w_1 x_1 + w_2 x_2 + b$.\n","\n","We now need to calculate $\\frac{\\partial L}{\\partial w} = \\frac{\\partial (y - \\hat{y})^2}{\\partial w}$. To expand this further, we utilize the chain rule as follows:\n","\n","$$\\frac{\\partial (y - \\hat{y})^2}{\\partial w} = \\frac{\\partial (y - \\hat{y})^2}{\\partial (y - \\hat{y})} \\frac{\\partial (y - \\hat{y})}{\\partial w} = 2(y - \\hat{y})\\frac{\\partial (y - \\hat{y})}{\\partial w} = 2(y - \\hat{y})(-1)\\frac{\\partial \\hat{y}}{\\partial w}$$\n","\n","We know that $\\hat{y}$ is given by $\\sigma(o)$, so we can utilize the chain rule again:\n","\n","$$\\frac{\\partial \\hat{y}}{\\partial w} = \\frac{\\partial \\sigma(o)}{\\partial w} = \\frac{\\partial \\sigma(o)}{\\partial o}\\frac{\\partial o}{\\partial w}$$\n","\n","The first partial derivative (the first derivative for the sigmoid function) is equal to $\\sigma(o)(1-\\sigma(o))$. The second partial derivative is equal to $x_1, x_2$ for $w=w_1, w_2$ and equal to 1 for $w=b$ (the bias weight).\n","\n","We can then use gradient descent to update the weights given a learning rate:\n","\n","$$w_{i,t+1} = w_{i,t} - \\eta \\frac{\\partial L}{\\partial w_i}$$\n","\n","Or, expanded based on our results above:\n","\n","$$w_{i,t+1} = w_{i,t} - \\eta \\left( 2(y - \\hat{y})(-1) \\sigma(o)(1-\\sigma(o)) x_i\\right)$$\n","\n","Which is equal to:\n","\n","$$w_{i,t+1} = w_{i,t} + \\eta \\cdot 2(y - \\hat{y})\\sigma(o)(1-\\sigma(o)) x_i$$\n","\n","If you want to go through a refresher on linear algebra, this link is a good recommendation: https://explained.ai/matrix-calculus/index.html"]},{"cell_type":"code","metadata":{"id":"zjS-Qxf3_Dkh","colab_type":"code","colab":{}},"source":["def train(instance, weights, y_true, l_rate):\n","  prediction = predict(instance, weights)\n","  abserror   = y_true - prediction\n","  weights[0] = weights[0] + l_rate * 2 * abserror * prediction * (1-prediction)\n","  for i in range(len(weights)-1):\n","      weights[i+1] = weights[i+1] + l_rate * 2 * abserror * prediction * (1-prediction) * instance[i]\n","  return weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-ZNhUv7A4ZJ","colab_type":"text"},"source":["Next, we can set our learning rate, and train for one pass over our instances."]},{"cell_type":"code","metadata":{"id":"ltpJJJ2K7ag-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1595328744901,"user_tz":-120,"elapsed":789,"user":{"displayName":"Seppe vanden Broucke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMpesh_oeuo7Fc2WsgK89OJRIeyn_p5F2LV-ineDc=s64","userId":"09380512084218149317"}},"outputId":"d7af28cf-52a6-4864-e8cd-58b8c50b8fa1"},"source":["l_rate = 0.01\n","\n","for i in range(len(X)):\n","  weights = train(X[i], weights, y[i], l_rate)\n","\n","print(weights)\n","\n","for i in range(len(X)):\n","  prediction = predict(X[i], weights)\n","  print(X[i], y[i], '->', prediction)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.00013804714817943972, 0.030374561900359753, -0.0043131276125743965]\n","[0, 1] 0 -> 0.49895623140008755\n","[1, 0] 0 -> 0.5076275604874748\n","[2, 2] 0 -> 0.5130622560931664\n","[3, 4] 0 -> 0.5184938648995636\n","[4, 2] 1 -> 0.5282224798656285\n","[5, 2] 1 -> 0.5357848625086622\n","[4, 1] 1 -> 0.5292971938385711\n","[5, 0] 1 -> 0.5379297045186441\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ri7r_w9GBHB_","colab_type":"text"},"source":["It looks like nothing has happened. So let's try this for a couple more \"epochs\" (passes over the training data) and see what happens:"]},{"cell_type":"code","metadata":{"id":"xF3mdQIo7JHG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1595328755376,"user_tz":-120,"elapsed":781,"user":{"displayName":"Seppe vanden Broucke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMpesh_oeuo7Fc2WsgK89OJRIeyn_p5F2LV-ineDc=s64","userId":"09380512084218149317"}},"outputId":"cf8492a3-6ebc-4a9f-a149-aebd414fde64"},"source":["l_rate = 0.01\n","epochs = 2000\n","\n","for n_epoch in range(epochs):\n","  for i in range(len(X)):\n","    weights = train(X[i], weights, y[i], l_rate)\n","\n","print(weights)\n","\n","for i in range(len(X)):\n","  prediction = predict(X[i], weights)\n","  print(X[i], y[i], '->', prediction)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-3.1220278679378843, 1.8204208827300656, -1.2193192699453246]\n","[0, 1] 0 -> 0.012851662514460058\n","[1, 0] 0 -> 0.21389468820743307\n","[2, 2] 0 -> 0.12788112224834713\n","[3, 4] 0 -> 0.0732339330104518\n","[4, 2] 1 -> 0.8482598018341195\n","[5, 2] 1 -> 0.9718440871193595\n","[4, 1] 1 -> 0.9498047670006002\n","[5, 0] 1 -> 0.9974777451273938\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"45d7AMpif6Eq","colab_type":"text"},"source":["Things to try:\n","\n","- Try playing around with the training instances (`X`) -- what happens if the values are very dispersed? Can you implement a normalization preprocessing step?\n","- Try changing the initialization of the weights, can you find a configuration which requires less training epochs?\n","- Try changing the learning rate. Can you implement an \"adaptive scheme\" where you change the learning rate over the epochs?\n","- Try implementing an alternative, simpler weight update function: $w_{i,t+1} = w_{i,t} + \\eta \\cdot (y - \\hat{y}) x_i$, why does this work as well?\n","- Can you find a bunch of training instances for which the perceptron clearly fails?"]},{"cell_type":"code","metadata":{"id":"LQS76hAjgbB0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}