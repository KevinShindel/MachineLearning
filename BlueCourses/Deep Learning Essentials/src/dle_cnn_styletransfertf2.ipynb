{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dle_cnn_styletransfertf2.ipynb","provenance":[{"file_id":"1H-lNMxVF9NPME6oZVphFRoi7yglJMZxP","timestamp":1588532524983}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2Rf4_vobPGqr","colab_type":"text"},"source":["This notebook illustrates how you would apply style transfer and custom optimization using TensorFlow 2 compatible code. This example has been adapted from Fran√ßois Chollet."]},{"cell_type":"code","metadata":{"id":"779onWqc7FXs","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.applications import vgg19\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.optimizers.schedules import ExponentialDecay\n","from matplotlib import pyplot as plt\n","import numpy as np\n","from PIL import Image\n","from skimage import io"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I1g48qHX_HYP","colab_type":"text"},"source":["These first few lines are the same as the other style transfer notebook."]},{"cell_type":"code","metadata":{"id":"ufn87qj37O5V","colab_type":"code","colab":{}},"source":["base_image_path  = 'http://www.petsexperience.com/wp-content/uploads/2018/05/What-Are-Hybrid-Cat-Breeds.jpg'\n","style_image_path = 'https://assets.saatchiart.com/saatchi/467/art/2717235/1787128-EQGJYLSZ-7.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_tBIxE27O0r","colab_type":"code","colab":{}},"source":["def preprocess_image(path, resize=False):\n","  image_original  = sk_load_img(path)\n","  image_processed = image_original.copy()\n","  if resize is not False: image_processed = image_processed.resize(resize)\n","  image_processed = image.img_to_array(image_processed)\n","  image_processed = np.expand_dims(image_processed, axis=0)\n","  image_processed = vgg19.preprocess_input(image_processed)\n","  return image_original, image_processed\n","\n","def deprocess_image(arr, size):\n","  x = np.copy(arr).reshape(size + (3, ))\n","  # VGG19 preprocess by normalizing each channel with the ImageNet means, so we just add these back up\n","  # Want to know where this comes from? See https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L135\n","  x[:, :, 0] += 103.939\n","  x[:, :, 1] += 116.779\n","  x[:, :, 2] += 123.68\n","  # We also need to convert back from (blue, green, red) to (red, green, blue)\n","  x = x[:, :, ::-1]\n","  # Finally, we clip all values between 0 and 255\n","  x = np.clip(x, 0, 255).astype('uint8')\n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AekOVAOV7Osj","colab_type":"code","colab":{}},"source":["img_ncols, img_nrows   = 800, 600"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0AMSTte7V0v","colab_type":"code","colab":{}},"source":["# Helper function to read an image from an URL\n","def sk_load_img(url):\n","  return Image.fromarray(io.imread(url))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VD7jLoMs7VyL","colab_type":"code","colab":{}},"source":["base_img_original, base_img_processed   = preprocess_image(base_image_path, (img_ncols, img_nrows))\n","style_img_original, style_img_processed = preprocess_image(style_image_path, (img_ncols, img_nrows))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwV2Z58m7kN0","colab_type":"code","colab":{}},"source":["# Difference between base and combined image, using SSE\n","def content_loss(base, combination):\n","  return K.sum(K.square(combination - base))\n","\n","# Difference between combined image and style image, using SSE on difference of their gram matrices, normalized\n","def style_loss(style, combination):\n","  assert K.ndim(style) == 3\n","  assert K.ndim(combination) == 3\n","  S = gram_matrix(style)\n","  C = gram_matrix(combination)\n","  channels = 3\n","  size = img_nrows * img_ncols\n","  return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n","\n","# Gram matrix: dot product between a matrix and its transposed one\n","def gram_matrix(x):\n","  assert K.ndim(x) == 3\n","  # Put the channels / depth in front and flatten to a 2d array\n","  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n","  gram = K.dot(features, K.transpose(features))\n","  return gram\n","\n","# Distance between neighboring pixels, without taking the border into account\n","def total_variation_loss(x, p=1.25):\n","  assert K.ndim(x) == 4\n","  a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n","  b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n","  return K.sum(K.pow(a + b, p))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJ2NSVnYky6m","colab_type":"text"},"source":["These lines are comparable as well: we load in a pretrained VGG19 model, and construct a quick helper function to get intermediate outputs."]},{"cell_type":"code","metadata":{"id":"39zRuptvTeEz","colab_type":"code","colab":{}},"source":["model = vgg19.VGG19(weights='imagenet', include_top=False)\n","\n","outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n","\n","feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aNOQDNcEk6Bk","colab_type":"text"},"source":["Now we see a couple of differences. We construct function here which returns our custom loss value given the three image sets."]},{"cell_type":"code","metadata":{"id":"criC-0vR_W0O","colab_type":"code","colab":{}},"source":["style_layer_names = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n","content_layer_name = 'block5_conv2'\n","\n","total_variation_weight = 1e-6\n","style_weight = 2e-6\n","content_weight = 2e-8\n","\n","def compute_loss(combination_image, base_image, style_reference_image):\n","  input_tensor = tf.concat([base_image,\n","                            style_reference_image,\n","                            combination_image], axis=0)\n","  features = feature_extractor(input_tensor)\n","\n","  # Initialize the loss\n","  loss = tf.zeros(shape=())\n","\n","  # Add content loss\n","  layer_features = features[content_layer_name]\n","  base_image_features = layer_features[0, :, :, :]\n","  combination_features = layer_features[2, :, :, :]\n","  loss = loss + content_weight * content_loss(base_image_features, combination_features)\n","  # Add style loss\n","  for layer_name in style_layer_names:\n","    layer_features = features[layer_name]\n","    style_reference_features = layer_features[1, :, :, :]\n","    combination_features = layer_features[2, :, :, :]\n","    sl = style_loss(style_reference_features, combination_features)\n","    loss += (style_weight / len(style_layer_names)) * sl\n","  \n","  # Add total variation loss\n","  loss += total_variation_weight * total_variation_loss(combination_image)\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TL8UdlblBzV","colab_type":"text"},"source":["Now for the main part. We use the `tf.function` decorator here so TensorFlow can compile this function to speed things up. This function returns the loss as well as the gradients. The latter is done using the `GradientTape` mechanism, which we have briefly visited before."]},{"cell_type":"code","metadata":{"id":"vn0MHIgF63LD","colab_type":"code","colab":{}},"source":["@tf.function\n","def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n","  with tf.GradientTape() as tape:\n","    loss = compute_loss(combination_image, base_image, style_reference_image)\n","  grads = tape.gradient(loss, combination_image)\n","  return loss, grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6c5szGo6lNJ8","colab_type":"text"},"source":["To perform our optimization, we won't use SciPy here but directly use one of the Keras optimizers (standard SGD in this case). Note that given our hyperparameters here, we need to run for a couple more iterations here. Also note the use of `optimizer.apply_gradients`, which is the actual call changing the combination image based on the gradients we have calculated."]},{"cell_type":"code","metadata":{"id":"F8aKlj4P-BX-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"185JxTOg3PSjCOJp0RH0g_5TFDnW6I0xb"},"executionInfo":{"status":"ok","timestamp":1588534905220,"user_tz":-120,"elapsed":1290730,"user":{"displayName":"Seppe vanden Broucke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMpesh_oeuo7Fc2WsgK89OJRIeyn_p5F2LV-ineDc=s64","userId":"09380512084218149317"}},"outputId":"7763fae8-c7dc-44a5-afa9-4e9433d01d5b"},"source":["optimizer = SGD(\n","  ExponentialDecay(initial_learning_rate=100., decay_steps=1000, decay_rate=0.96)\n",")\n","\n","combination_image = tf.Variable(base_img_processed.copy())\n","\n","iterations = 5000\n","\n","for i in range(iterations):\n","  loss, grads = compute_loss_and_grads(combination_image, base_img_processed, style_img_processed)\n","  optimizer.apply_gradients([(grads, combination_image)])\n","  if i % 100 == 0:\n","    print('Iteration', i, ' loss = ', loss)\n","    combined_image_deprocessed = deprocess_image(combination_image.numpy(), (img_nrows, img_ncols))\n","    plt.figure(figsize=(4, 5))\n","    plt.imshow(combined_image_deprocessed)\n","    plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"lauuC_Yc9D2C","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}