{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:29.999204200Z",
     "start_time": "2026-01-27T17:28:29.982206700Z"
    }
   },
   "source": [
    "# Learn plan:\n",
    "# - Character RNN\n",
    "# - Stateless RNN\n",
    "# - Stateful RNN"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Libraries",
   "id": "5d0094dd3af480fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:35.677860800Z",
     "start_time": "2026-02-05T10:47:26.519764800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras.utils\n",
    "import numpy as np\n",
    "from keras.src.legacy.preprocessing.text import Tokenizer # TODO: seems this a deprecated class, find out a new way to do this\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, optimizers, losses, metrics, callbacks"
   ],
   "id": "7f31113ee9fb2c18",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loading",
   "id": "e712833a38d04e9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:37.416279400Z",
     "start_time": "2026-01-27T17:28:37.269402200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shakespear_url = 'https://github.com/karpathy/char-rnn'\n",
    "\n",
    "file_path = keras.utils.get_file('shakespear.txt', shakespear_url)\n",
    "\n",
    "with open(file_path) as f:\n",
    "    shakespear_text = f.read()"
   ],
   "id": "e138b3d37f1a157f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:37.821425200Z",
     "start_time": "2026-01-27T17:28:37.419211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(char_level=True)  # coding at char level\n",
    "tokenizer.fit_on_texts(shakespear_text) # fit tokenizer on text"
   ],
   "id": "e9f427e54cb3d52e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:37.868650800Z",
     "start_time": "2026-01-27T17:28:37.821425200Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.texts_to_sequences(['First'])",
   "id": "364a6a5063b2ede6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37, 7, 14, 9, 2]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:37.900322800Z",
     "start_time": "2026-01-27T17:28:37.868650800Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.sequences_to_texts([[37, 7, 14, 9, 2]])",
   "id": "370c6d1f2b69da8b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:37.950203400Z",
     "start_time": "2026-01-27T17:28:37.900322800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_id = len(tokenizer.word_index)\n",
    "max_id # total unique characters in the text"
   ],
   "id": "d9e7744a88a4b1e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:37.996054200Z",
     "start_time": "2026-01-27T17:28:37.950203400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_size = tokenizer.document_count\n",
    "data_size # total characters in the text"
   ],
   "id": "47182ff5dffbdc2a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preparation",
   "id": "ec99a03081342e45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:38.112289400Z",
     "start_time": "2026-01-27T17:28:37.998052600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's encode the entire text\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([\n",
    "    shakespear_text\n",
    "])) - 1  # shift to zero-based\n",
    "encoded"
   ],
   "id": "5cc2d21cfe4d44a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37, 37, 37, ..., 29, 37, 37], shape=(309117,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:28:38.154908600Z",
     "start_time": "2026-01-27T17:28:38.115295400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Split for Training / Validation\n",
    "train_size = data_size * 90 // 100\n",
    "train_size"
   ],
   "id": "f1995f1552d2132d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278205"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:34:18.218849700Z",
     "start_time": "2026-01-27T17:34:18.050805300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "batch_size = 32\n",
    "\n",
    "# Start from encoded characters\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# Sliding windows (each element is now a Dataset, not a tensor)\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "# Turn each window Dataset into a tensor of shape [window_length]\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Now split into input (all but last char) and target (shifted by 1)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[1:]))\n",
    "\n",
    "# Batch and one-hot encode\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "\n",
    "dataset = dataset.prefetch(1) # prefetch next batch while training on the current one"
   ],
   "id": "3191aabb7ec3c2a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Preparation ( Char-Rnn )",
   "id": "8c355b796d172ea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:34:55.152452200Z",
     "start_time": "2026-01-27T17:34:54.906210700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    layers.InputLayer(shape=[None, max_id]),\n",
    "    layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.TimeDistributed(layers.Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    metrics=[metrics.sparse_categorical_accuracy,\n",
    "             metrics.sparse_top_k_categorical_accuracy,\n",
    "             metrics.sparse_categorical_crossentropy],\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "id": "42104933d36a668a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (\u001B[38;5;33mGRU\u001B[0m)                       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)      │        \u001B[38;5;34m78,336\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001B[38;5;33mGRU\u001B[0m)                     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)      │        \u001B[38;5;34m99,072\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m74\u001B[0m)       │         \u001B[38;5;34m9,546\u001B[0m │\n",
       "│ (\u001B[38;5;33mTimeDistributed\u001B[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,072</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,546</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m186,954\u001B[0m (730.29 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,954</span> (730.29 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m186,954\u001B[0m (730.29 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">186,954</span> (730.29 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ],
   "id": "8ab391c280875e7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T17:40:01.968708100Z",
     "start_time": "2026-01-27T17:34:56.776354100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(dataset, # Takes so long time! ( ~ 10h)\n",
    "          epochs=20,\n",
    "          callbacks=[\n",
    "                     early_stopping_callback,\n",
    "                     ])"
   ],
   "id": "61ae7e7c57bb579d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   1218/Unknown \u001B[1m305s\u001B[0m 242ms/step - loss: 2.2813 - sparse_categorical_accuracy: 0.3961 - sparse_categorical_crossentropy: 2.2813 - sparse_top_k_categorical_accuracy: 0.6478"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "7abf5f4de0166eea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accuracy = model.evaluate(dataset)\n",
    "print(f\"Accuracy: {accuracy[1]*100:.2f}%\")"
   ],
   "id": "3cb4c1eb38fb252c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prediction example",
   "id": "68b3944eb1009783"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_input(text):\n",
    "    \" Preprocess input text into one-hot encoded format \"\n",
    "    X = np.array(tokenizer.texts_to_sequences([text])) - 1\n",
    "    return tf.one_hot(X, depth=max_id)"
   ],
   "id": "65d669f3d6cadc22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_new = preprocess_input(\"ROMEO:\") # input text\n",
    "Y_pred = model.predict_classes(X_new) # predict next characters\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # decode predicted characters\n"
   ],
   "id": "a9b7187c14a484d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text generation",
   "id": "703273b0229cbd74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def next_char(text, temperature=1.0):\n",
    "    \" Generate the next character given the input text and temperature \"\n",
    "    X_new = preprocess_input([text])\n",
    "    y_proba = model.predict(X_new)[0, -1, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(\n",
    "        logits=tf.expand_dims(rescaled_logits, 0),\n",
    "        num_samples=1\n",
    "    ) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=100, temperature=1.0):\n",
    "    \" Generate text by predicting next characters iteratively \"\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "id": "ea7193692df2dac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "complete_text(\"t\", temperature=0.2) # the belly the great and who shall be the belly the\n",
    "\n",
    "# For best results, create model with recurrent_dropout=0.3, add more layers with GRU"
   ],
   "id": "491b40e9d80ba26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stateful RNN\n",
    "> - Make sense only when previous data is related to current data ( e.g. time series, text )\n",
    "> - Stateful RNN maintain hidden states between batches"
   ],
   "id": "e3d3cdc42edcf3fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size]) # Start from encoded characters\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True) # Sliding windows\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length)) # Turn each window Dataset into a tensor of shape [window_length]\n",
    "dataset = dataset.batch(1) # Batch size = 1 to maintain state between batches\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:,1:])) # Now split into input (all but last char) and target (shifted by 1)\n",
    "dataset = dataset.map( # Batch and one-hot encode\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch)\n",
    ")\n",
    "dataset = dataset.prefetch(1) # prefetch next batch while training on the current one"
   ],
   "id": "db5405f42c6622f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Sequential([\n",
    "    layers.InputLayer(batch_input_shape=[batch_size, None, max_id]),\n",
    "    layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2), # stateful=True - for stateful RNN\n",
    "    layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.TimeDistributed(layers.Dense(max_id, activation='softmax'))\n",
    "])"
   ],
   "id": "b65544ee4a2d7ffb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ResetStateCallback(callbacks.Callback):\n",
    "    \" Custom callback to reset states at the end of each epoch \"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.reset_states()"
   ],
   "id": "452262e79179f75e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.sparse_categorical_crossentropy,\n",
    "    metrics=[metrics.sparse_categorical_accuracy],\n",
    ")\n",
    "\n",
    "model.fit(dataset,\n",
    "          epochs=20,\n",
    "          callbacks=[\n",
    "                     early_stopping_callback,\n",
    "                     ResetStateCallback()\n",
    "                     ])"
   ],
   "id": "f8b04fdf341d938e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentiment Analysis with RNN ( IMBD Dataset )",
   "id": "bf802655940bb913"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:41.206327700Z",
     "start_time": "2026-02-05T10:47:35.714749300Z"
    }
   },
   "cell_type": "code",
   "source": "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data() # load IMDB dataset",
   "id": "6543a77ecb011a7e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\username\\Projects\\MachineLearning\\.venv\\Lib\\site-packages\\numpy\\lib\\_format_impl.py:838: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  array = pickle.load(fp, **pickle_kwargs)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:41.304297800Z",
     "start_time": "2026-02-05T10:47:41.208335200Z"
    }
   },
   "cell_type": "code",
   "source": "X_train[0][:10] # first 10 word ids of first review",
   "id": "6cdba33208e2d56c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:41.456592700Z",
     "start_time": "2026-02-05T10:47:41.334208200Z"
    }
   },
   "cell_type": "code",
   "source": "word_index = tf.keras.datasets.imdb.get_word_index() # get word to id mapping",
   "id": "e8caec4fd7edfd90",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:41.554315600Z",
     "start_time": "2026-02-05T10:47:41.458604900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id_to_word = {id + 3: word for word, id in word_index.items()} # shift by 3 to leave space for special tokens\n",
    "for idx, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[idx] = token\n",
    "\" \".join([id_to_word[idx] for idx in X_train[0][:10]])"
   ],
   "id": "48437e953c20b977",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:43.158054300Z",
     "start_time": "2026-02-05T10:47:41.555846800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "imdb_dataset, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "train_size = info.splits['train'].num_examples\n",
    "train_size"
   ],
   "id": "9524dfc795406ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:43.205923600Z",
     "start_time": "2026-02-05T10:47:43.189306800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(X_batch, y_batch ):\n",
    "    \"\"\"\n",
    "    Preprocess text data: tokenize, pad/truncate to fixed length\n",
    "    For speedup training we take only first 300 characters of each review\n",
    "    \"\"\"\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300) # truncate to max length\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \") # remove HTML tags\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"[^a-zA-Z']\", b\" \") # keep only letters and apostrophes\n",
    "    X_batch = tf.strings.split(X_batch) # tokenize\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n"
   ],
   "id": "ff8bd7befbb8d0b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:54.593190Z",
     "start_time": "2026-02-05T10:47:43.209937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "\n",
    "for X_batch, y_batch in imdb_dataset['train'].batch(32).map(preprocess): # build vocabulary\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(review.numpy())"
   ],
   "id": "762484b2fd489884",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:54.638695200Z",
     "start_time": "2026-02-05T10:47:54.595248300Z"
    }
   },
   "cell_type": "code",
   "source": "vocabulary.most_common()[:10]",
   "id": "4172fcd1a581a9ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309),\n",
       " (b'the', 61137),\n",
       " (b'a', 38564),\n",
       " (b'of', 33983),\n",
       " (b'and', 33431),\n",
       " (b'to', 27707),\n",
       " (b'I', 27019),\n",
       " (b'is', 25719),\n",
       " (b'in', 18966),\n",
       " (b'this', 18490)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:54.724322200Z",
     "start_time": "2026-02-05T10:47:54.669240500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lets create a vocabulary wit h a most common 10000 words\n",
    "vocab_size = 10000 # vocabulary size\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common(vocab_size)] # keep only most common words\n",
    "words = tf.constant(truncated_vocabulary) # convert to tensor\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64) # assign unique id to each word\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids) # create initializer for lookup table\n",
    "num_oov_buckets = 1000 # add extra buckets for out-of-vocabulary words (words not in the truncated vocabulary)\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets) # create lookup table"
   ],
   "id": "85f662074e886e73",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:54.813190800Z",
     "start_time": "2026-02-05T10:47:54.729886100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table.lookup(\n",
    "    tf.constant(\n",
    "        [b'This movie was faaaaaantastic'.split()]\n",
    "    )\n",
    ") # If words are found there are ids below 10000, else above 10000"
   ],
   "id": "a9f512d8f5ed0a19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:54.923456Z",
     "start_time": "2026-02-05T10:47:54.816243200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    \" Encode words to their corresponding ids using the lookup table \"\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = imdb_dataset['train'].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ],
   "id": "b98f3d0e1130ce36",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:47:55.138911700Z",
     "start_time": "2026-02-05T10:47:54.951054900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build a model\n",
    "\n",
    "embed_size = 128 # embedding size\n",
    "\n",
    "model = Sequential([\n",
    "    layers.InputLayer(shape=(None,)),\n",
    "    layers.Embedding(input_dim=vocab_size + num_oov_buckets,\n",
    "                     output_dim=embed_size,\n",
    "                     mask_zero=True),\n",
    "    layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.GRU(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss=losses.binary_crossentropy,\n",
    "    metrics=[metrics.binary_accuracy],\n",
    ")"
   ],
   "id": "43d6149288efb8d5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T10:51:08.033326800Z",
     "start_time": "2026-02-05T10:48:32.584882700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=1,\n",
    "    # validation_data=test_set,\n",
    "    # callbacks=[early_stopping_callback]\n",
    ")"
   ],
   "id": "fc84d04152778253",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m782/782\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m155s\u001B[0m 185ms/step - binary_accuracy: 0.7002 - loss: 0.5645\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Masking",
   "id": "6fdb2cf1ec49abab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "K = tf.keras.backend\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(None, 5))\n",
    "mask = tf.keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = tf.keras.layers.Embedding(vocab_size+num_oov_buckets, embed_size)(inputs)\n",
    "z = tf.keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = tf.keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(z)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ],
   "id": "61493d958156deca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Re-using pretrained embeddings",
   "id": "8e51d2bda9e41114"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T11:11:50.842959500Z",
     "start_time": "2026-02-05T11:11:49.451246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_layer = hub.KerasLayer(\"https://www.kaggle.com/models/google/nnlm/TensorFlow2/tf2-preview-en-dim50/1\", output_shape=[50],\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ],
   "id": "2bfac6c292ea03e2",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x0000026B97D93890> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      3\u001B[39m hub_layer = hub.KerasLayer(\u001B[33m\"\u001B[39m\u001B[33mhttps://www.kaggle.com/models/google/nnlm/TensorFlow2/tf2-preview-en-dim50/1\u001B[39m\u001B[33m\"\u001B[39m, output_shape=[\u001B[32m50\u001B[39m], \n\u001B[32m      4\u001B[39m                            input_shape=[], dtype=tf.string)\n\u001B[32m      6\u001B[39m model = keras.Sequential()\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhub_layer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m model.add(keras.layers.Dense(\u001B[32m16\u001B[39m, activation=\u001B[33m'\u001B[39m\u001B[33mrelu\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m      9\u001B[39m model.add(keras.layers.Dense(\u001B[32m1\u001B[39m, activation=\u001B[33m'\u001B[39m\u001B[33msigmoid\u001B[39m\u001B[33m'\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Projects\\MachineLearning\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:97\u001B[39m, in \u001B[36mSequential.add\u001B[39m\u001B[34m(self, layer, rebuild)\u001B[39m\n\u001B[32m     95\u001B[39m         layer = origin_layer\n\u001B[32m     96\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(layer, Layer):\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     98\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mOnly instances of `keras.Layer` can be \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     99\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33madded to a Sequential model. Received: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    100\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m(of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(layer)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    101\u001B[39m     )\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._is_layer_name_unique(layer):\n\u001B[32m    103\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    104\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAll layers added to a Sequential model \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    105\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mshould have unique names. Name \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer.name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m is already \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    106\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mthe name of a layer in this model. Update the `name` argument \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    107\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mto pass a unique name.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    108\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x0000026B97D93890> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Loading",
   "id": "b7a841a397d41f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "train_size = info.splits['train'].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets['train'].batch(batch_size).prefetch(1)\n"
   ],
   "id": "b1138a17c7048a18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "ecb618fc0bf9ebfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=1,\n",
    ")"
   ],
   "id": "6eea1859b23950bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61de1ad3a107e41a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
