{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "housing_df = pd.read_csv('dataset/housing.csv')"
   ],
   "id": "6e1b4d0b51df29a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the first 5 rows of the dataset\n",
    "housing_df.head(5)"
   ],
   "id": "cdc447c3608ef615",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "housing_df.info() # total_bedrooms has missing values",
   "id": "a76bb77efab781c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# creating histograms \n",
    "housing_df.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ],
   "id": "88b9adc74c055453",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "def split_train_test(data, test_ratio) -> tuple:\n",
    "    \"\"\" \n",
    "    Split the data into training and testing sets\n",
    "    data: the dataset to be split\n",
    "    test_ratio: the ratio of the test set size to the total dataset size\n",
    "    \"\"\"\n",
    "    np.random.seed(42) # to ensure that the same test set is generated each time\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = split_train_test(housing_df, 0.2) # 80% training and 20% testing"
   ],
   "id": "e4741624ed3f5f89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# hashed version of the split_train_test function\n",
    "\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_df_with_id = housing_df.reset_index() # adds an 'index' column\n",
    "housing_df_with_id['id'] = housing_df['longitude'] * 1000 + housing_df['latitude'] # creates a unique identifier\n",
    "train_set, test_set = split_train_test_by_id(housing_df_with_id, 0.2, 'index')"
   ],
   "id": "d3909c07ebb0b06b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using the sklearn train_test_split function\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing_df, test_size=0.2, random_state=42)\n"
   ],
   "id": "934125c1563c4794",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stratified sampling\n",
    "housing_df['income_cat'] = pd.cut(housing_df['median_income'], # create an income category attribute\n",
    "                                  bins=[0., 1.5, 3.0, 4.5, 6., np.inf], # income categories\n",
    "                                  labels=[1, 2, 3, 4, 5]) # labels for the income categories\n",
    "\n",
    "housing_df['income_cat'].hist() # income category histogram"
   ],
   "id": "78dc7e0ba61a7455",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stratified sampling using the income category\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(housing_df, housing_df['income_cat']):\n",
    "    strat_train_set = housing_df.loc[train_index]\n",
    "    strat_test_set = housing_df.loc[test_index]"
   ],
   "id": "da9110a5f487b416",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# income category proportions in the test set\n",
    "res = strat_test_set['income_cat'].value_counts() / len(strat_test_set)\n",
    "res"
   ],
   "id": "10da5040302c3eb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove the income_cat attribute to return the data to its original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('income_cat', axis=1, inplace=True)"
   ],
   "id": "b179e78afa4379a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualizing the data\n",
    "housing = strat_train_set.copy()\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1) # alpha is used to visualize the density of the data points"
   ],
   "id": "5b2ec8b0f83b4ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# colored by the housing prices\n",
    "housing.plot(kind='scatter', # scatter plot\n",
    "             x='longitude', # x-axis\n",
    "             y='latitude', # y-axis\n",
    "             alpha=0.4, # transparency\n",
    "             s=housing['population']/100, # radius of each circle represents the district's population\n",
    "             label='population', # legend label\n",
    "             figsize=(10,7), # figure size\n",
    "             c='median_house_value', # color represents the price\n",
    "             cmap=plt.get_cmap('jet'), # color map\n",
    "             colorbar=True) # color bar\n",
    "# price of house very much related to the location and population density"
   ],
   "id": "b9db13aa55ed1eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# colored map of california\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "california_img = mpimg.imread('images/california.png')\n",
    "\n",
    "ax = housing.plot(kind='scatter', x='longitude', y='latitude', figsize=(10,7),\n",
    "                    s=housing['population']/100, label='population',\n",
    "                    c='median_house_value', cmap=plt.get_cmap('jet'),\n",
    "                    colorbar=False, alpha=0.4)\n",
    "plt.imshow(california_img, # image to be displayed\n",
    "           extent=(-124.55, -113.80, 32.45, 42.05), # coordinates of the image\n",
    "           alpha=0.5, # transparency\n",
    "           cmap=plt.get_cmap('jet')) # color map\n"
   ],
   "id": "65ab74dac91b25b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Looking for correlations\n",
    "housing_df['ocean_proximity'] = housing_df['ocean_proximity'].astype('category').cat.codes\n",
    "corr_matrix = housing_df.corr()\n",
    "res = corr_matrix['median_house_value'].sort_values(ascending=False)\n",
    "res"
   ],
   "id": "ee224d3d0a28a7b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# scatter matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))"
   ],
   "id": "5158c60ae836e9d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# median_house_value vs median_income\n",
    "\n",
    "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
   ],
   "id": "4979c7e5f71339bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# feature engineering\n",
    "\n",
    "housing['rooms_per_household'] = housing['total_rooms'] / housing['households'] # rooms per household\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms'] / housing['total_rooms'] # bedrooms per room\n",
    "housing['population_per_household'] = housing['population'] / housing['households'] # population per household"
   ],
   "id": "5a9fb53bf7af067f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# correlation matrix with the newly added features\n",
    "housing['ocean_proximity'] = housing['ocean_proximity'].astype('category').cat.codes\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ],
   "id": "70b122a180a3fc62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# data preparation\n",
    "\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()"
   ],
   "id": "a3daa6dc26ca2b74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# data cleaning\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy.testing as npt\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "housing_num = housing.drop('ocean_proximity', axis=1) # drop the non-numeric attribute\n",
    "\n",
    "imputer.fit(housing_num) # fit the imputer instance to the training data\n",
    "\n",
    "npt.assert_array_equal(imputer.statistics_, housing_num.median().values) # imputer statistics"
   ],
   "id": "9514bc20c5c502c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# transform the training set by replacing missing values with the learned medians\n",
    "X = imputer.transform(housing_num) # numpy array returns\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) # convert to pandas dataframe"
   ],
   "id": "24ba9da9688363e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# handling text and categorical attributes\n",
    "\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "housing_cat.head(10)"
   ],
   "id": "8ddce3a7b8b81be6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert the text categories to numbers\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10] # encoded categories\n",
    "ordinal_encoder.categories_ # categories\n",
    "# When categories are similar (e.g. 'good', 'average', 'bad'), ordinal encoding is useful, but when they are not, one-hot encoding is better !!!"
   ],
   "id": "f5f240b708bfea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# let's use one-hot encoding\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "housing_cat_1hot = one_hot_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot.toarray() # one-hot encoded categories"
   ],
   "id": "e4f1438cfac77514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "one_hot_encoder.categories_ # categories",
   "id": "673549ba8f6633f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# custom transformers\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 # column indices\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room: # add bedrooms_per_room if active\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else: # else return the original data\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ],
   "id": "71d5e504c663cf8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values) # add the new attributes\n",
    "housing_extra_attribs"
   ],
   "id": "22a3cf8ab8f342f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# transformation pipelines\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')), # imputer\n",
    "    ('attribs_adder', CombinedAttributesAdder()), # add new attributes\n",
    "    ('std_scaler', StandardScaler()) # standardize the data\n",
    "])"
   ],
   "id": "478304031e377dde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create a full pipeline that handles both numerical and categorical attributes\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attribs), # numerical attributes\n",
    "    ('cat', OneHotEncoder(), cat_attribs) # categorical attributes\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing) # transform the data"
   ],
   "id": "bc18c06d310a5d88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# select model and train\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "lin_reg_model.fit(housing_prepared, housing_labels) # fit the model\n",
    "\n",
    "# prediction\n",
    "some_data = housing.iloc[:5] # select some data from the training set\n",
    "some_labels = housing_labels.iloc[:5] # select some labels from the training set\n",
    "some_data_prepared = full_pipeline.transform(some_data) # transform the data"
   ],
   "id": "ebbe8fd1be3aadb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predicted_labels = lin_reg_model.predict(some_data_prepared) # predict\n",
    "original_labels = list(some_labels) # original labels\n",
    "diffs = predicted_labels - some_labels # differences between the predicted and original labels\n",
    "\n",
    "print('Predicted labels:', predicted_labels)\n",
    "print('Original labels:', original_labels)\n",
    "print('Differences:', diffs)"
   ],
   "id": "9bfd35e95cc4d895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# measure model by using Mean Squared Error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg_model.predict(housing_prepared) # predict the labels\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions) # calculate the mean squared error\n",
    "lin_rmse = np.sqrt(lin_mse) # calculate the root mean squared error\n",
    "lin_rmse # root mean squared error is 68.628 dollars per house"
   ],
   "id": "5b725fe70ebe2b11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use DecisionTreeRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg_model = DecisionTreeRegressor()\n",
    "\n",
    "tree_reg_model.fit(housing_prepared, housing_labels) # fit the model\n",
    "housing_predictions = tree_reg_model.predict(housing_prepared) # predict the labels\n",
    "\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions) # calculate the mean squared error\n",
    "tree_rmse = np.sqrt(tree_mse) # calculate the root mean squared error\n",
    "tree_rmse # root mean squared error is 0 dollars per house ( overfitting, model works well on the training data but not on the test data)"
   ],
   "id": "51e728abe426cc69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using cross-validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg_model, housing_prepared, housing_labels, # cross-validation\n",
    "                            scoring='neg_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores) # root mean squared errors\n",
    "print('Scores:', tree_rmse_scores)\n",
    "print('Mean:', tree_rmse_scores.mean()) # mean of the root mean squared errors 71510 dollars per house (that's higher than the linear regression model)\n",
    "print('Standard deviation:', tree_rmse_scores.std())"
   ],
   "id": "7b4934f3fa586ddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using cross-validation with the linear regression model\n",
    "\n",
    "lin_scores = cross_val_score(lin_reg_model, housing_prepared, housing_labels, # cross-validation\n",
    "                            scoring='neg_mean_squared_error', cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores) # root mean squared errors\n",
    "\n",
    "print('Scores:', lin_rmse_scores)\n",
    "print('Mean:', lin_rmse_scores.mean()) # mean of the root mean squared errors 69052 dollars per house\n",
    "print('Standard deviation:', lin_rmse_scores.std())"
   ],
   "id": "68518491284f3cf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# using RandomForestRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg_model = RandomForestRegressor()\n",
    "\n",
    "forest_reg_model.fit(housing_prepared, housing_labels) # fit the model\n",
    "\n",
    "housing_predictions = forest_reg_model.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse # root mean squared error is 18655 dollars per house"
   ],
   "id": "885ed345d8a52b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "forest_scores = cross_val_score(forest_reg_model, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "\n",
    "print('Scores:', forest_rmse_scores)\n",
    "print('Mean:', forest_rmse_scores.mean()) # Mean value is 50.267 dollars per house\n",
    "print('Standard deviation:', forest_rmse_scores.std()) # Standard deviation is 2.177 dollars"
   ],
   "id": "83ba204417f1a291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If cross_val_score greater then mse of predicted data, it means overfitting\n",
    "# If cross_val_score less then mse of predicted data, it means underfitting"
   ],
   "id": "199c5c6f76dd175d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save the model\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(forest_reg_model, 'forest_reg_model.pkl') # save the model"
   ],
   "id": "23723d19c29422a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the model\n",
    "\n",
    "forest_reg_model_loaded = joblib.load('forest_reg_model.pkl') # load the model"
   ],
   "id": "bb27d087bf102b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fine-tuning the model\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, # hyperparameters\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]} # hyperparameters\n",
    "]\n",
    "\n",
    "forest_reg_model = RandomForestRegressor()\n",
    "\n",
    "grid_search = RandomizedSearchCV(forest_reg_model, param_grid, cv=5, # grid search\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            return_train_score=True, verbose=1, n_jobs=-1)    \n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels) # fit the model"
   ],
   "id": "a4998270d290a2fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_params = grid_search.best_params_ # best hyperparameters\n",
    "print('Best parameters:', best_params)\n",
    "best_estimator = grid_search.best_estimator_ # best estimator"
   ],
   "id": "c20b76b80ec269de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Best parameters: {'n_estimators': 30, 'max_features': 6}\n",
    "# That's means n_estimators selected maximum value, so you can try to increase n_estimators"
   ],
   "id": "9fc2191b5d3a515a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [100, 150, 200, 300], 'max_features': [5, 6, 7], 'bootstrap': [False]}\n",
    "]\n",
    "grid_search = RandomizedSearchCV(forest_reg_model, param_grid, cv=5, # grid search\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            return_train_score=True, verbose=1, n_jobs=-1)    \n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels) # fit the model\n",
    "\n",
    "best_params = grid_search.best_params_ # best hyperparameters\n",
    "print('Best parameters:', best_params)"
   ],
   "id": "a7693f7884497753",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# analyzing the best models and their errors\n",
    "\n",
    "feature_importances = best_estimator.feature_importances_ # feature importances\n",
    "print('Feature importances:', feature_importances)\n",
    "\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room'] # extra attributes\n",
    "cat_encoder = full_pipeline.named_transformers_['cat'] # categorical encoder\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0]) # one-hot encoded categories\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs # attributes\n",
    "sorted(zip(feature_importances, attributes), reverse=True) # sorted feature importances"
   ],
   "id": "80f695383b00c424",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# evaluate the model on the test set\n",
    "\n",
    "final_model = grid_search.best_estimator_ # best model\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1) # test set\n",
    "y_test = strat_test_set['median_house_value'].copy() # test labels\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test) # transform the test set\n",
    "final_predictions = final_model.predict(X_test_prepared) # predict the labels\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions) # calculate the mean squared error\n",
    "final_rmse = np.sqrt(final_mse) # calculate the root mean squared error\n",
    "print('Root mean squared error:', final_rmse) # root mean squared error is "
   ],
   "id": "e6614b5b0a5c9afb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# confidence interval calculation\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "confidence = 0.95 # confidence level\n",
    "\n",
    "squared_errors = (final_predictions - y_test) ** 2 # squared errors\n",
    "np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, # confidence interval\n",
    "                        loc=squared_errors.mean(),\n",
    "                        scale=stats.sem(squared_errors)))   # confidence interval"
   ],
   "id": "8d52a4ff3a8dad79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Question parts",
   "id": "fe5f0552829b4b7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Use SVC with various hyperparameters\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.]},\n",
    "    {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "     'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]}\n",
    "]\n",
    "\n",
    "svr_model = SVR()\n",
    "\n",
    "grid_search = GridSearchCV(svr_model, param_grid, cv=5, # grid search\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            return_train_score=True, verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels) # fit the model\n",
    "\n",
    "best_params = grid_search.best_params_ # best hyperparameters\n",
    "print('Best parameters:', best_params) # {'C': 30000.0, 'kernel': 'linear'}"
   ],
   "id": "d4c5e9bb75642be0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Replace GridSearchCV with RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "# Note: gamma is ignored when kernel is \"linear\"\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000), # reciprocal distribution generator\n",
    "        'gamma': expon(scale=1.0), # exponential distribution generator\n",
    "    }\n",
    "\n",
    "svr_model = SVR()\n",
    "\n",
    "randomized_search = RandomizedSearchCV(svr_model, param_grid, cv=5, # randomized search\n",
    "                                       n_iter=50, random_state=42,\n",
    "                                       scoring='neg_mean_squared_error',\n",
    "                                       return_train_score=True, verbose=1, n_jobs=-1)  \n",
    "\n",
    "negative_mse = randomized_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse\n",
    "# {'C': 157055.10989448498, 'gamma': 0.26497040005002437, 'kernel': 'rbf'}"
   ],
   "id": "e3a539726db7ed35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Question: Try adding a transformer in the preparation pipeline to select only the most important attributes.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def indices_of_top_k(arr, k):\n",
    "    \"\"\"\n",
    "    Return the indices of the top k elements of an array\n",
    "    \"\"\"\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Select the top k features based on the feature importances\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_importances, k: int):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer\n",
    "        \"\"\"\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data\n",
    "        \"\"\"\n",
    "        return X[:, self.feature_indices_]\n",
    "    \n",
    "k = 5 # number of top features\n",
    "top_k_feature_indices = indices_of_top_k(feature_importances, k) # top k feature indices\n",
    "\n",
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])\n",
    "\n",
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing) # transform the data"
   ],
   "id": "21fa1f95ad3e987a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Question: Try creating a single pipeline that does the full data preparation plus the final prediction.\n",
    "\n",
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    ('prediction', final_model)\n",
    "])\n",
    "\n",
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels) # fit the model"
   ],
   "id": "7b07a2a542f43af1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Question: Automatically explore some preparation options using GridSearchCV.\n",
    "\n",
    "param_grid = [{\n",
    "    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'], # imputer strategies\n",
    "    'feature_selection__k': list(range(1, len(feature_importances) + 1)) # number of top features (1 to max number of features)\n",
    "}]\n",
    "\n",
    "grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n",
    "                                scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "grid_search_prep.fit(housing, housing_labels) # fit the model\n",
    "\n",
    "grid_search_prep.best_params_ # best parameters {'feature_selection__k': 15,  'preparation__num__imputer__strategy': 'most_frequent'}"
   ],
   "id": "efb4f625920c766e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
