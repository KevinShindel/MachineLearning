{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "32ee6e443cff0b75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:13:10.129648Z",
     "start_time": "2024-11-14T19:13:06.692383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ],
   "id": "bc82e2b8fc436719",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:13:17.891585Z",
     "start_time": "2024-11-14T19:13:10.132427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ],
   "id": "aa4ba447aea573a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:13:17.898856Z",
     "start_time": "2024-11-14T19:13:17.893324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape # 70k images, 784 features (28x28 pixels)"
   ],
   "id": "4b543ebce43aa0d7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "some_digit = X.loc[0]\n",
    "some_digit_image = some_digit.values.reshape(28, 28) # create image from 1D array\n",
    "plt.imshow(some_digit_image, cmap=\"binary\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "id": "91ce7b948cce18d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# label\n",
    "print(y[0])\n",
    "print(type(y[0]))"
   ],
   "id": "dbb14e66d02d5c11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:13:17.907368Z",
     "start_time": "2024-11-14T19:13:17.900905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert string to int\n",
    "y = y.astype(np.uint8)"
   ],
   "id": "da9bf78f889a48bd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T19:13:17.915076Z",
     "start_time": "2024-11-14T19:13:17.909007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ],
   "id": "7416d8a27ffdc746",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# binary classifier\n",
    "\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ],
   "id": "b7d9111926b78319",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sgd_clf.predict([some_digit]) # predict the digit 5 (True is 5, False is not 5)",
   "id": "ad18fd977edf798c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# cross validation check\n",
    "score = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "print(score)"
   ],
   "id": "226857869272f9bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# cross validation check with custom implementation\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.loc[train_index]\n",
    "    y_train_folds = y_train_5.loc[train_index]\n",
    "    X_test_fold = X_train.loc[test_index]\n",
    "    y_test_fold = y_train_5.loc[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ],
   "id": "c2d8d756d6a4d78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dummy classifier\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "    \n",
    "never_5_clf = Never5Classifier()\n",
    "\n",
    "score = cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "print(score) # [0.91125 0.90855 0.90915]\n",
    "\n",
    "# It's means only 10% of the data is 5, so if you always guess that the image is not 5, you will be right 90% of the time."
   ],
   "id": "c526fb57aec8bed4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# confusion matrix\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "confusion_matrix = pd.crosstab(y_train_5, y_train_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(confusion_matrix)\n",
    "# True Negative: 53892, False Positive: 687\n",
    "# False Negative: 1891, True Positive: 3530\n",
    "\n",
    "# accuracy = TN + TP / (TN + FP + FN + TP) = 53892 + 3530 / (53892 + 687 + 1891 + 3530) = 0.9502 %\n",
    "# precision = TP / (TP + FP) = 3530 / (3530 + 687) = 0.84  \n",
    "# recall = TP / (TP + FN) = 3530 / (3530 + 1891) = 0.65 %\n",
    "# f1 = 2 * (precision * recall) / (precision + recall) = 2 * (0.84 * 0.65) / (0.84 + 0.65) = 0.74 % "
   ],
   "id": "e55910439d498995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# precision and recall ratio\n",
    "# we cannot change the threshold of the model, but we can change the threshold of the decision function\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "print(y_scores)\n",
    "\n",
    "threshold = 0 # default threshold\n",
    "y_some_digit_pred = (y_scores > threshold) # True\n",
    "print(y_some_digit_pred)\n",
    "\n",
    "# make the threshold higher\n",
    "threshold = 8000\n",
    "\n",
    "y_some_digit_pred = (y_scores > threshold) # False\n",
    "print(y_some_digit_pred)\n"
   ],
   "id": "ea8e9a16537ffb24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ],
   "id": "219ce2ec78b9e125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "print('Threshold for 90 % accuracy is: ', threshold_90_precision)\n",
    "\n",
    "x_train_pred_90 = (y_scores >= threshold_90_precision)\n",
    "\n",
    "pre_score = precision_score(y_train_5, x_train_pred_90)\n",
    "rec_score = recall_score(y_train_5, x_train_pred_90)\n",
    "\n",
    "print(pre_score) # 0.90 % precision increased\n",
    "print(rec_score) # 0.48 %  recall decreased"
   ],
   "id": "3d8a9a4941ccf8ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ROC curve ( indicate the true positive rate against the false positive rate)\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label) # plot the curve\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # plot the diagonal\n",
    "    plt.axis([0, 1, 0, 1]) # set the axis\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "# The more recall of model, the more false positive rate"
   ],
   "id": "15bff3601459ff81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ROC AUC score\n",
    "\n",
    "# AUC score is the area under the ROC curve\n",
    "roc_auc_score = roc_auc_score(y_train_5, y_scores)\n",
    "print(roc_auc_score) # 0.96 \n",
    "# Good model has ROC AUC score close to 1, bad model has ROC AUC score close to 0.5"
   ],
   "id": "a891a09a4fc52fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Random Forest Classifier model for comparison\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")\n",
    "\n",
    "y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)\n",
    "\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ],
   "id": "f83a1b838fe16751",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ROC AUC score\n",
    "roc_auc_score(y_train_5, y_scores_forest) # 0.99"
   ],
   "id": "5bec2a64bcb1bb7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Multiclass classification\n",
    "\n",
    "# SGD, RandomForest, and Naive Bayes classifiers can be used for multiclass classification\n",
    "# But SVM, Linear classifiers, and Neural Networks are binary classifiers it can be used for multiclass classification using OvR or OvO strategy\n",
    "\n",
    "# SVC bad for large datasets, so we use SGDClassifier\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train) # y_train, not y_train_5 (so it's multiclass classification)"
   ],
   "id": "5a09ccf3afb5dcc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prediction = svm_clf.predict([some_digit]) # predict the digit 5\n",
    "# SVC classifier trained 10 binary classifiers, get their decision scores for the image, and selected the class with the highest score\n",
    "digit_scores = svm_clf.decision_function([some_digit])\n",
    "print(digit_scores)\n",
    "print(np.argmax(digit_scores)) # 5\n",
    "print(prediction)\n",
    "len(svm_clf.classes_) # 10 classes"
   ],
   "id": "b0a58a0d0d744a6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Using OvR strategy\n",
    "ovr_clf = OneVsRestClassifier(SVC())\n",
    "\n",
    "ovr_clf.fit(X_train, y_train)\n",
    "\n",
    "prediction = ovr_clf.predict([some_digit])  \n",
    "print(prediction) # 5\n",
    "\n",
    "len(ovr_clf.estimators_) # 10"
   ],
   "id": "8b7376c50590af2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training SGDClassifier\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "prediction = sgd_clf.predict([some_digit])\n",
    "print(prediction) # predicted - 3"
   ],
   "id": "9ed079354c896e99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sgd_clf.decision_function([some_digit]) # 3\n",
    "\n",
    "score = cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "print(score) # \n",
    "print(score.mean()) #"
   ],
   "id": "25d9658624001037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# To improve the accuracy, we can scale the inputs\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "score = cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n",
    "print(score) # \n",
    "print(score.mean()) #"
   ],
   "id": "a15d70c44ca91107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Error Analysis\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ],
   "id": "c3555b95044e43e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "np.fill_diagonal(norm_conf_mx, 0) # fill the diagonal with 0\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray) # plot the matrix\n",
    "plt.show()"
   ],
   "id": "60980dd35228f421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Draw the digit 3 and 5\n",
    "\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.values.reshape(size, size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = 'binary', **options)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "plt.save_fig(\"error_analysis_digits_plot\")\n",
    "plt.show()"
   ],
   "id": "36b0460ea4957fcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Multilabel classification\n",
    "y_train_large = (y_train >= 7) # is the digit large?\n",
    "y_train_odd = (y_train % 2 == 1) # is the digit odd?\n",
    "\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "\n",
    "knn_clf.predict([some_digit]) # [False, True] 3 is not large and odd"
   ],
   "id": "20d4f170923a19bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\") # F1 score\n",
    "# If we have more large digits, we can set the weight of the large digits to be higher\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"weighted\") # F1 score (adjust weight)"
   ],
   "id": "3662029bd72119c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Multioutput classification\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "noise = np.random.randint(0, 100, (len(X_train), 784)) # generate random noise\n",
    "X_train_mod = X_train + noise # add noise to the training set\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784)) # generate random noise\n",
    "X_test_mod = X_test + noise # add noise to the test set\n",
    "y_train_mod = X_train # target is the original image\n",
    "y_test_mod = X_test # target is the original image\n",
    "\n",
    "some_index = 0\n",
    "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
    "plt.subplot(122); plot_digit(y_test_mod[some_index])\n",
    "save_fig(\"noisy_digit_example_plot\")\n",
    "plt.show()\n",
    "\n",
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod.iloc[0]]) # clean the image\n",
    "plot_digits(clean_digit)"
   ],
   "id": "fdbe30fe5f7797bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Extra material\n",
    "\n",
    "1. Improve the model KNN to 97% accuracy (use grid search to find the best hyperparameters: weights and n_neighbors)\n",
    "2. Create shift-function to shift the image (make 4 copies of the image, each shifted by one pixel)\n",
    "3. Complete Titanic project on Kaggle\n",
    "4. Create a spam classifier (Apache SpamAssassin public datasets) https://homl.info/spamassassin"
   ],
   "id": "4051433373b9e2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Task 1 - Improve the model KNN to 97% accuracy\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_neighbors': range(1, 31),\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "rnd_search = RandomizedSearchCV(knn_clf, param_dist, n_iter=3, cv=3, scoring='accuracy', random_state=42, verbose=2, n_jobs=-1)\n",
    "rnd_search.fit(X_train, y_train)\n",
    "\n",
    "print(rnd_search.best_params_)\n",
    "print(rnd_search.best_score_)"
   ],
   "id": "7070cc7c04728a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-14T19:16:35.712589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Task 2 - Create shift-function to shift the image\n",
    "# TODO: Run on Desktop\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import shift\n",
    "\n",
    "def shift_image(image, dx, dy):\n",
    "    image = image.reshape((28, 28))\n",
    "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
    "    return shifted_image.reshape([-1])\n",
    "\n",
    "# Convert X_train and y_train to NumPy arrays\n",
    "X_train_np = X_train.values\n",
    "y_train_np = y_train.values\n",
    "\n",
    "# Preallocate arrays for augmented data\n",
    "num_shifts = 4\n",
    "X_train_augmented = np.zeros((len(X_train_np) * (num_shifts + 1), 784))\n",
    "y_train_augmented = np.zeros(len(y_train_np) * (num_shifts + 1))\n",
    "\n",
    "# Copy original data\n",
    "X_train_augmented[:len(X_train_np)] = X_train_np\n",
    "y_train_augmented[:len(y_train_np)] = y_train_np\n",
    "\n",
    "# Apply shifts\n",
    "shift_idx = len(X_train_np)\n",
    "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
    "    for i in range(len(X_train_np)):\n",
    "        X_train_augmented[shift_idx] = shift_image(X_train_np[i], dx, dy)\n",
    "        y_train_augmented[shift_idx] = y_train_np[i]\n",
    "        shift_idx += 1\n",
    "\n",
    "# Shuffle the augmented dataset\n",
    "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
    "X_train_augmented = X_train_augmented[shuffle_idx]\n",
    "y_train_augmented = y_train_augmented[shuffle_idx]"
   ],
   "id": "66038d245df6dc73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knn_clf = KNeighborsClassifier(**{'weights': 'distance', 'n_neighbors': 3})\n",
    "knn_clf.fit(X_train_augmented, y_train_augmented)\n",
    "\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "print(accuracy)"
   ],
   "id": "793413257b95a92f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# 3. Titanic challenge -> Kaggle/Challenges/Titanic Disaster.ipynb",
   "id": "78936562e1f0fde9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Create a spam classifier\n",
    "\n",
    "easy_ham_folder = 'dataset/easy_ham'\n",
    "spam_folder = 'dataset/spam'\n",
    "\n",
    "ham_filenames = [name for name in sorted(os.listdir(easy_ham_folder)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(spam_folder)) if len(name) > 20]\n",
    "\n",
    "len(ham_filenames), len(spam_filenames)"
   ],
   "id": "11a563b5786a1080",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_email(filename, spam_path):\n",
    "    with open(os.path.join(spam_path, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ],
   "id": "b9b619dc42f0fd1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ham_emails = [load_email(name, easy_ham_folder) for name in ham_filenames]\n",
    "spam_emails = [load_email(name, spam_folder) for name in spam_filenames]"
   ],
   "id": "c3204a3db7838f2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(ham_emails[1].get_content().strip())",
   "id": "1af3d71fa9c518bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_email_structure(email):\n",
    "    '''Return the email structure'''\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ],
   "id": "30e559666f658aa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ],
   "id": "842225286276113e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "structures_counter(ham_emails).most_common()",
   "id": "c59a502874525b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "structures_counter(spam_emails).most_common()",
   "id": "5f3378e828343496",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "    print(header,\":\",value)"
   ],
   "id": "f6b0b368be3351a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spam_emails[0][\"Subject\"]",
   "id": "ec78d2265231b6c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "cbe1ea60d31aec1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ],
   "id": "236611941981c1d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1] if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ],
   "id": "ed4212323a0a671e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ],
   "id": "1a1e621ea59a6aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(email_to_text(sample_html_spam)[:100], \"...\")",
   "id": "1db96cc50caee4e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ],
   "id": "c75e70cc0403c4ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ],
   "id": "c7b0dbafbd8ae86b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, strip_headers=True,\n",
    "                 lower_case=True,\n",
    "                 remove_punctuation=True,\n",
    "                 replace_urls=True,\n",
    "                 replace_numbers=True,\n",
    "                 stemming=True):\n",
    "        \n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ],
   "id": "c6c9f62544373bd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ],
   "id": "b96e0f3e5527ec00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ],
   "id": "870d40ef7dd48429",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ],
   "id": "d6447a1ccab2ada5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_few_vectors.toarray()",
   "id": "dd95c3051020eabc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "vocab_transformer.vocabulary_",
   "id": "d4ef5c5fecfe7da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ],
   "id": "c08709827e6500e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ],
   "id": "f85173cf0b36c85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "print(\"F1 Score: {:.2f}%\".format(100 * f1_score(y_test, y_pred)))"
   ],
   "id": "eb1a75e6b36d22a3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
