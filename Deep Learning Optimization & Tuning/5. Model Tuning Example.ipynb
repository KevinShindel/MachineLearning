{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Importing the required libraries\n",
    "from src.utils import base_model_config, create_and_run_model, plot_graph, build_model, get_rca_data"
   ],
   "id": "d56aae2396b535ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Acquire and process data\n",
    "X, y = get_rca_data()"
   ],
   "id": "885e0feedb1c94cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Tuning layers in the network\n",
    "\n",
    "accuracy_measures = {}\n",
    "layer_list =[]\n",
    "for layer_count in range(1,6):\n",
    "    \n",
    "    #32 nodes in each layer\n",
    "    layer_list.append(32)\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_rca_data()\n",
    "    \n",
    "    model_config[\"HIDDEN_NODES\"] = layer_list\n",
    "    model_name = \"Layers-\" + str(layer_count)\n",
    "    history=create_and_run_model(model_config,X,Y,model_name)\n",
    "    \n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "    \n",
    "plot_graph(accuracy_measures,\"Accuracy vs Layers\")"
   ],
   "id": "fe7802b6449cdc68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tuning the number of nodes in the network\n",
    "accuracy_measures = {}\n",
    "\n",
    "node_increment=8\n",
    "\n",
    "for node_count in range(1,5):\n",
    "    \n",
    "    #have 2 hidden layers in the networks as selected above\n",
    "    layer_list =[]\n",
    "    for layer_count in range(2):\n",
    "        layer_list.append(node_count * node_increment)\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_rca_data()\n",
    "    \n",
    "    model_config[\"HIDDEN_NODES\"] = layer_list\n",
    "    model_name = \"Nodes-\" + str(node_count * node_increment)\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "\n",
    "plot_graph(accuracy_measures,\"Accuracy vs Nodes\")"
   ],
   "id": "5ca6127250c75aa3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tuning backpropagation",
   "id": "5eaaf547bf29ad01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use optimizers\n",
    "\n",
    "accuracy_measures = {}\n",
    "\n",
    "optimizer_list = ['sgd','rmsprop','adam','adagrad']\n",
    "for optimizer in optimizer_list:\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_rca_data()\n",
    "    \n",
    "    model_config[\"OPTIMIZER\"] = optimizer\n",
    "    model_name = \"Optimizer-\" + optimizer\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "    \n",
    "plot_graph(accuracy_measures,\"Accuracy vs Optimizers\")"
   ],
   "id": "e4a77ada47884ef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tuning learning rate\n",
    "\n",
    "accuracy_measures = {}\n",
    "\n",
    "learning_rate_list = [0.001, 0.005,0.01,0.1,0.5]\n",
    "for learning_rate in learning_rate_list:\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_rca_data()\n",
    "    \n",
    "    #Fix Optimizer to the one chosen above\n",
    "    model_config[\"OPTIMIZER\"]=\"rmsprop\"\n",
    "    model_config[\"LEARNING_RATE\"] = learning_rate\n",
    "    model_name=\"Learning-Rate-\" + str(learning_rate)\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    #Using validation accuracy\n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "    \n",
    "plot_graph(accuracy_measures,\"Accuracy vs Learning Rate\")"
   ],
   "id": "5be38ce3adf6ad9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Avoiding overfitting",
   "id": "5d5193ef791f1e1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tuning regularizers\n",
    "accuracy_measures = {}\n",
    "\n",
    "regularizer_list = [l1(0.01), l2(0.01), l1_l2(l1=0.01, l2=0.01)]\n",
    "regularizer_names = ['l1', 'l2', 'l1_l2']\n",
    "for regularizer, reg_name in zip(regularizer_list, regularizer_names):\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_rca_data()\n",
    "    \n",
    "    model_config[\"REGULARIZER\"] = regularizer\n",
    "    model_name = \"Regularizer-\" + str(reg_name)\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    #Switch to validation accuracy\n",
    "    accuracy_measures[model_name] = history.history[\"val_accuracy\"]\n",
    "\n",
    "plot_graph(accuracy_measures,\"Validation Accuracy vs Regularizers\")"
   ],
   "id": "4a22b8cd050e6356",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tuning dropout\n",
    "accuracy_measures = {}\n",
    "\n",
    "dropout_list = [0.0, 0.1, 0.2, 0.5]\n",
    "for dropout in dropout_list:\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_rca_data()\n",
    "    \n",
    "    #Use the regularizer chosen above\n",
    "    model_config[\"REGULARIZER\"] = \"l2\"\n",
    "    model_config[\"DROPOUT_RATE\"] = dropout\n",
    "    model_name=\"Dropout-\" + str(dropout)\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    #Using validation accuracy\n",
    "    accuracy_measures[model_name] = history.history[\"val_accuracy\"]\n",
    "    \n",
    "plot_graph(accuracy_measures,\"Validation Accuracy vs Dropout\")"
   ],
   "id": "a4758cd2c506586c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.python.keras import callbacks\n",
    "from tensorflow.keras import models as k_models\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adagrad\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "def build_model(feature_shape=(7,),\n",
    "                hidden_nodes=[32], \n",
    "                optimizer='adam',\n",
    "                learning_rate=0.001, \n",
    "                dropout_rate=0.0,\n",
    "                kernel_regularizer=None,\n",
    "                normalization=None, \n",
    "                kernel_initializer=None, \n",
    "                bias_initializer=None):\n",
    "    \n",
    "    model = k_models.Sequential()\n",
    "    model.add(Input(feature_shape))\n",
    "    for nodes in hidden_nodes:\n",
    "        model.add(Dense(nodes,\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        bias_initializer=bias_initializer,\n",
    "                        ))\n",
    "        if normalization and normalization == \"batch\":\n",
    "                model.add(BatchNormalization())\n",
    "        if dropout_rate > 0.0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    if optimizer == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adagrad':\n",
    "        opt = Adagrad(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Acquire and process data\n",
    "X, y = get_rca_data()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Wrap the model using KerasRegressor\n",
    "dnn = KerasRegressor(model=build_model, verbose=1)\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'model__hidden_nodes': [[16], [32, 32], [64, 64], [128, 128]],\n",
    "    'model__optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad'],\n",
    "    'model__learning_rate': [0.001, 0.005, 0.01],\n",
    "    'model__kernel_regularizer': [None, l1(0.01), l2(0.01), l1_l2(l1=0.01, l2=0.01)],\n",
    "    'model__dropout_rate': [0.0, 0.2, 0.5],\n",
    "    'model__normalization': [None, 'batch'],\n",
    "    'epochs': [5, 10, 15, 25, 30],\n",
    "    'model__bias_initializer': ['zeros', 'ones', 'glorot_uniform'],\n",
    "    'model__kernel_initializer': ['glorot_uniform', 'he_normal', 'he_uniform']\n",
    "}\n",
    "\n",
    "rcv = RandomizedSearchCV(estimator=dnn,\n",
    "                         param_distributions=param_grid,\n",
    "                         scoring='neg_mean_absolute_error',\n",
    "                         cv=5,\n",
    "                         n_iter=10,\n",
    "                         verbose=1)\n",
    "\n",
    "history = rcv.fit(X_train, y_train, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best: %f using %s\" % (history.best_score_, history.best_params_)) \n",
    "# Best: -0.392004 \n",
    "# Best params using {'model__optimizer': 'adam',\n",
    "# 'model__normalization': None, \n",
    "# 'model__learning_rate': 0.01, \n",
    "# 'model__kernel_regularizer': L2, \n",
    "# 'model__kernel_initializer': 'he_normal',\n",
    "# 'model__hidden_nodes': [64, 64],\n",
    "# 'model__dropout_rate': 0.2,\n",
    "# 'model__bias_initializer': 'zeros',\n",
    "# 'epochs': 15}"
   ],
   "id": "2a1bdc169b581f8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = history.predict(X_test) # values is float, need to make it integer type by ceil method\n",
    "y_pred = np.round(y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "score = cross_val_score(history, X_test, y_test, cv=5, scoring='accuracy')\n",
    "print(f\"Cross Validation Score: {score.mean()}\") # 0.79 % accuracy\n",
    "\n",
    "print(f\"Accuracy: {acc}\") # 0.79 % accuracy"
   ],
   "id": "39ad1366b53f63c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a1679ecaa970351",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
