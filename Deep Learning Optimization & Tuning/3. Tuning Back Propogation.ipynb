{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tuning Back Propagation\n",
    "\n",
    "Description: "
   ],
   "id": "c4c1f3721c725d1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Vanishing and exploding gradients\n",
    "> \n",
    "> - The delta computed should be the right size for gradual descent\n",
    "> - Too small -> Decay, no changed to weights\n",
    "> - Too big -> Choppy with no descend "
   ],
   "id": "c5c82f8c97a71817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> **Gradient Descent**\n",
    "> \n",
    "> - The gradient descent algorithm multiplies the gradient by a scalar known as the learning rage to determine the next point\n",
    "> \n",
    "> **Solution**\n",
    "> \n",
    "> - Weight initialization\n",
    "> - Activation functions\n",
    "> - Batch normalization"
   ],
   "id": "a46242f775d9a539"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Normalize the inputs each hidden layer\n",
    "- Center and scale (StandardScaler)\n",
    "- Normalize the inputs to be the same scale\n",
    "- Helps attain higher accurate with lower epochs\n",
    "- Additional computation and increased inference times"
   ],
   "id": "1382aa3a6d33cbc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils import base_model_config\n",
    "from src.utils import create_and_run_model\n",
    "from src.utils import plot_graph\n",
    "from src.utils import get_data"
   ],
   "id": "642a454e30c2a8d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Batch Normalization Experiment\n",
    "\n",
    "accuracy_measures = {}\n",
    "\n",
    "normalization_list = ['none','batch']\n",
    "for normalization in normalization_list:\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_data()\n",
    "    \n",
    "    model_config[\"NORMALIZATION\"] = normalization\n",
    "    model_name=\"Normalization-\" + normalization\n",
    "    history=create_and_run_model(model_config,X,Y,model_name)\n",
    "    \n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "    \n",
    "plot_graph(accuracy_measures, 'Batch Normalization')"
   ],
   "id": "cc06b7883d03dc3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### After running the experiment, we obtain the following results:\n",
    "- The model which uses batch normalization has boost in accuracy on early epochs\n",
    "- Optimal epochs are reduced (max 4-6 epochs)"
   ],
   "id": "4e1277134db803b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Optimizers\n",
    "\n",
    "> Description:\n",
    "> \n",
    "> - Regular gradient descent can be slow\n",
    "> - Takes a lot of time to get closer to the desired accuracy\n",
    "> - More training time and resources\n",
    "> - Limited training data may also impact gradient descent\n",
    "> - Optimizers help speed up the training process\n",
    "> - Changes the delta value to get closer to desired state\n",
    "\n",
    "**Available Optimizers**\n",
    "\n",
    "1. SGD (Stochastic Gradient Descent) - best for shallow networks\n",
    "2. RMSprop (Root Mean Square Propagation) - best for deep networks\n",
    "3. Adam (Adaptive Moment Estimation) - best for deep networks\n",
    "4. Adagrad (Adaptive Gradient Algorithm) - best for sparse data"
   ],
   "id": "89e14539ef99a0ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimizer experiment\n",
    "\n",
    "accuracy_measures = {}\n",
    "\n",
    "optimizer_list = ['sgd','rmsprop','adam','adagrad']\n",
    "for optimizer in optimizer_list:\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_data()\n",
    "    \n",
    "    model_config[\"OPTIMIZER\"] = optimizer\n",
    "    model_name = \"Optimizer-\" + optimizer\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "\n",
    "plot_graph(accuracy_measures, 'Optimizers')"
   ],
   "id": "bf9938d50512f630",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### After running the experiment, we obtain the following results:\n",
    "1. The Adam optimizer performs better than the rest at early epochs (but unstable)\n",
    "2. Rmsprop optimizer is the second best (more stable)"
   ],
   "id": "8aa129582118562e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Learning Rate\n",
    "\n",
    "> Description:\n",
    "> 1. Rate at witch the weights are change in response to the estimated error\n",
    "> 2. Works in conjunction with the optimizer\n",
    "> 3. Numeric value used to adjust the delta computed\n",
    "\n",
    "\n",
    "**Learning Rate Selection**\n",
    "- Large learning rate -> Faster learning with fewer epochs, Risk of exploding gradients\n",
    "- Small learning rate -> Slower learning, Risk of vanishing gradients"
   ],
   "id": "21f4f5a938296ca1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Learning Rate Experiment\n",
    "\n",
    "accuracy_measures = {}\n",
    "\n",
    "learning_rate_list = [0.001, 0.005,0.01,0.1,0.5]\n",
    "for learning_rate in learning_rate_list:\n",
    "    \n",
    "    model_config = base_model_config()\n",
    "    X,Y = get_data()\n",
    "    \n",
    "    model_config[\"LEARNING_RATE\"] = learning_rate\n",
    "    model_name=\"Learning-Rate-\" + str(learning_rate)\n",
    "    history=create_and_run_model(model_config,X,Y, model_name)\n",
    "    \n",
    "    #accuracy\n",
    "    accuracy_measures[model_name] = history.history[\"accuracy\"]\n",
    "\n",
    "plot_graph(accuracy_measures, \"Compare Learning Rates\")"
   ],
   "id": "2382a14b7429a0d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### After running the experiment, we obtain the following results:\n",
    "\n",
    "1. Learning rate of 0.01 provide boost accuracy on early epochs (but not stable)\n",
    "2. Learning rate of 0.005 is the best for the model (more stable)"
   ],
   "id": "9f598567ff73856b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
