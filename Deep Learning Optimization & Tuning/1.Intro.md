# Intro

## Regularization

- **Regularization** is a technique used to prevent overfitting in a model. It is a form of regression that shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.
- **L1 Regularization**: L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. It is also known as Lasso (Least Absolute Shrinkage and Selection Operator).
- **L2 Regularization**: L2 regularization adds a penalty equal to the square of the magnitude of coefficients. It is also known as Ridge regression.
- **L1/L2 Regularization**: L1/L2 regularization adds both penalties to the loss function.
- **Elastic Net Regularization**: Elastic Net regularization is a linear combination of L1 and L2 regularization.
- **Dropout**: Dropout is a regularization technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.
- **Early Stopping**: Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. The idea is to stop the training process before the learner passes that point.
- **Data Augmentation**: Data augmentation is a technique used to artificially increase the size of a dataset by adding slightly modified copies of already existing data or newly created synthetic data from existing data.
- **Batch Normalization**: Batch normalization is a technique used to improve the training of deep learning models. It normalizes the output of each layer to have a mean of zero and a standard deviation of one. This helps to stabilize and speed up the training process.

## Prerequisites

Deep Learning: Background
- Vast domain with a variety of algorithms and techniques
- Multiple courses exists for concepts and implementations
- math requirements and coverage depth
- Tools - understanding and depth

## Scope of the Course

- **Deep Learning Optimization and Tuning** - layers and experiments
- **Optimization** - tuning hyperparameters
- **Tuning** - tuning  and best practice
- **Keras** - mask complexity
- **Minimal math coverage** - deeper topics omitted
- **Simple examples** - small datasets, easy understanding

## Course Prerequisites
- ML concepts and technologies
- Deep Learning concepts
- Python 3.8+ programming, Jupyter Notebooks
- Keras and TensorFlow

## Recommended Courses
- [Deep Learning: Getting Started](https://www.linkedin.com/learning/deep-learning-getting-started/getting-started-with-deep-learning?u=2113185)
- [Building Deep Learning Models with Keras 2.0](https://www.linkedin.com/learning/building-deep-learning-applications-with-keras/reshaping-the-world-with-deep-learning?u=2113185)
- [Building and Deploying Deep Learning Applications with TensorFlow](https://www.linkedin.com/learning/building-and-deploying-deep-learning-applications-with-tensorflow/)

## Exercise Files

- [GitHub Repository](https://www.linkedin.com/ambry/?x-li-ambry-ep=AQIvq_tN7kJiDQAAAY6d0B_y2CqwmwzrHXawT-XLkZyYuypJI_QH6qpvy5l_SgAbHVMDGhQjYJJDQYYQ_o465p7bbBl2cmM97VsKbPwFuD_v6Zu9HWM0YcHX5li3PQKMfkQegSG3kr_SOWJUlLn1KQM9DI6-N1bJJFT0QSwFkIWc7PUuLmGTZsZadlYcT8FIf5fEwXkVSg4rx4-BiRoQVrcK3r6Z9IQ-1nmczBelsIqTnX4EGWWqixeDOYfxT8pKXzMJ_tnHQEmlwVzxWIBczJOI6qlh63YOzKYpZFUwg_HEze-CAGjb_dFIT0BcVxSmNypbJTufupTqD7zZOPFLacOWVhAU9vfmx0PwFnC1f3ceD7I_CEp-n_v-lKakLq60MXMl76MN2uxbRB23Tlb6U7gNbaRjMUtfHEBOkgOT9ZT-BNDATySD94UFivx5uXpStiVszkUkt6C6ES7o0XI_Jgekkv7S1woj0jzunbTlI3RVdT2CRDfUecKcu_IG)