{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection must be used before hyperparameter tuning and model selection\n",
    "\n",
    "- [1. Feature variance threshold method](#1.-Lowest-Variance-threshold-method)\n",
    "- [2. K-best features method](#2.-K-best-features-method)\n",
    "- [3. Recursive Feature Elimination (RFE) method](#3.-Recursive-Feature-Elimination-(RFE)-method)\n",
    "- [4. Recursive Feature Elimination with Cross-Validation (RFECV) method](#4.-Recursive-Feature-Elimination-with-Cross-Validation-(RFECV)-method)\n",
    "- [5. Boruta method](#5.-Boruta-method)"
   ],
   "id": "a518cef7f5e1da87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Libraries",
   "id": "a06ea300f8751865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from boruta import BorutaPy\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE, RFECV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "id": "7e8713a2591a88fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Settings",
   "id": "809e6f81e864b8b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.rcParams['figure.figsize'] = (12, 9)",
   "id": "1d86ddad044272cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Dataset",
   "id": "15a09eba0a0a880f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wine_data = load_wine()",
   "id": "9bf0c5f6f760ff7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read Data",
   "id": "8e4d363a7cb05457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wine_df = pd.DataFrame(data=wine_data.data, columns=wine_data.feature_names)\n",
    "wine_df['target'] = wine_data.target"
   ],
   "id": "7900e5a5600a3382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wine_df.head(10)",
   "id": "c9a72ff6704bfa15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_to_plot = pd.melt(wine_df[['alcohol', 'malic_acid', 'alcalinity_of_ash', 'target']],\n",
    "                       id_vars='target',\n",
    "                       var_name='features',\n",
    "                       value_name='value')\n",
    "\n",
    "sns.swarmplot(data=data_to_plot, x='features', y='value', hue='target')\n",
    "plt.title('Feature Distribution by Target Class')\n",
    "plt.show()"
   ],
   "id": "f5d00eb06b165c7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wine_df['target'].value_counts() # distribution of target classes",
   "id": "32f67c84216f6353",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bar Plot Example for Categorical Feature",
   "id": "e3688c3991423d4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = [0,1,2]\n",
    "y = [59,71,48]\n",
    "\n",
    "ax.bar(x, y, width=0.2)\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['0', '1', '2'], fontsize=12)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index, y=value + 1, s=str(value), ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9ffac1d425cf2933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train/Test Split",
   "id": "9ca7658785820409"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X, y = wine_df.drop(columns=['target']), wine_df['target']",
   "id": "a05564bc2b102a94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ],
   "id": "b9e73ac9e61454ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline model: Gradient Boosting Classifier with all features",
   "id": "74f42b76cf4b127b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gbc_model = GradientBoostingClassifier(random_state=42, n_estimators=5)\n",
    "\n",
    "gbc_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gbc_model.predict(X_test)\n",
    "\n",
    "f1_gbc_score = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f1_gbc_score)"
   ],
   "id": "c20b054e6d59cd89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Lowest Variance threshold method",
   "id": "f107e0c76b47148b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_v1, X_test_v1, y_train_v1, y_test_v1 = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ],
   "id": "75dba7b8b498e218",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# calculate variance for each feature\n",
    "X_train_v1.var(axis=0)"
   ],
   "id": "c1b1138ae32d46ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If feature variance has not the same scale as the target variable, we need to scale it first\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_v1_scaled = pd.DataFrame(scaler.fit_transform(X_train_v1), columns=X_train_v1.columns)"
   ],
   "id": "49e2dcbd182b780e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = X.columns\n",
    "y = X_train_v1_scaled.var(axis=0)\n",
    "\n",
    "ax.bar(x, y, width=0.5)\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.set_ylim(0, 0.1)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index, y=value + 0.001, s=f\"{value:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "plt.title('Feature Variance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "c441ce9ae560c0d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sel_X_train_v1 = X_train_v1.drop(columns=['magnesium', 'ash'], axis=1)\n",
    "sel_X_test_v1 = X_test_v1.drop(columns=['magnesium', 'ash'], axis=1)\n",
    "\n",
    "gbc_model_v1 = GradientBoostingClassifier(random_state=42, n_estimators=5)\n",
    "gbc_model_v1.fit(sel_X_train_v1, y_train_v1)\n",
    "\n",
    "y_pred_v1 = gbc_model_v1.predict(sel_X_test_v1)\n",
    "\n",
    "f1_gbc_v1_score = f1_score(y_test_v1, y_pred_v1, average='weighted')\n",
    "\n",
    "print(f\"F1 Score with Variance Threshold: {f1_gbc_v1_score:.4f}\")"
   ],
   "id": "d59d5a1d634a4085",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x= ['All Features', 'Variance Threshold']\n",
    "y = [f1_gbc_score, f1_gbc_v1_score]\n",
    "\n",
    "ax.bar(x, y, width=0.4, color=['blue', 'orange'])\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlabel('Feature Selection Method')\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index, y=value + 0.02, s=f\"{value:.4f}\", ha='center', fontsize=12)\n",
    "\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "18b1d6a0db307e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. K-best features method\n",
    "- use a measure of importance to select the top K features"
   ],
   "id": "d7a5f02f71dd514e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Select metrics criteria for K-best features method\n",
    "\n",
    "| Target/Features | Categorical                            | Numerical                                                        |\n",
    "|-----------------|----------------------------------------|------------------------------------------------------------------|\n",
    "| Categorical     | Chi2 <br/> Mutual Info                 | t-test<br/> Mutual Info                                          |\n",
    "| Numerical       | ANOVA, <br/> t-test, <br/> Mutual Info | Pearson Correlation <br/> Spearman`s rank corr <br/> Mutual Info |"
   ],
   "id": "99c1091c538273b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ],
   "id": "c908df72f0cac6e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "f1_score_list = []\n",
    "\n",
    "for k in range(1, X_train_v2.shape[1]+1):\n",
    "\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=k) # our case is classification with numerical features\n",
    "\n",
    "    X_train_v2_selected = selector.fit_transform(X_train_v2, y_train_v2)\n",
    "    X_test_v2_selected = selector.transform(X_test_v2)\n",
    "\n",
    "    gbc_model_v2 = GradientBoostingClassifier(random_state=42, n_estimators=5)\n",
    "    gbc_model_v2.fit(X_train_v2_selected, y_train_v2)\n",
    "\n",
    "    y_pred_v2 = gbc_model_v2.predict(X_test_v2_selected)\n",
    "\n",
    "    f1_gbc_v2_score = f1_score(y_test_v2, y_pred_v2, average='weighted')\n",
    "\n",
    "    f1_score_list.append(f1_gbc_v2_score)"
   ],
   "id": "23531f03e0554fbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = list(range(1, X_train_v2.shape[1]+1))\n",
    "y = f1_score_list\n",
    "\n",
    "ax.bar(x, y, width=0.2)\n",
    "ax.set_xlabel('Number of Selected Features (K)')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x, fontsize=12)\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index + 1, y=value + 0.02, s=f\"{value:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title('F1 Score vs Number of Selected Features (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a157ef59cfb7a19f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# at leash 3 features are needed to reach the baseline score\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=3)\n",
    "\n",
    "X_train_v2_selected = selector.fit_transform(X_train_v2, y_train_v2)\n",
    "selected_feature_mask = selector.get_support()\n",
    "selected_features = X_train_v2.columns[selected_feature_mask]\n",
    "print('Most Important Features (K=3):')\n",
    "print(\"Selected Features:\", selected_features.tolist())"
   ],
   "id": "cba219c29954a0ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Recursive Feature Elimination (RFE) method\n",
    "- use an external model to assign weights to features and eliminate the least important ones recursively"
   ],
   "id": "2a39409f90217edb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_v3, X_test_v3, y_train_v3, y_test_v3 = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ],
   "id": "58097eb098d11c81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rfe_f1_score_list = []\n",
    "\n",
    "for k in range(1, X_train_v3.shape[1]+1):\n",
    "\n",
    "    rfe = RFE(estimator=GradientBoostingClassifier(random_state=42,\n",
    "                                                   n_estimators=5),\n",
    "              n_features_to_select=k, # number of features to select\n",
    "              step=1,\n",
    "              verbose=0) # for large datasets, use step>1 to speed up the process\n",
    "\n",
    "    X_train_v3_selected = rfe.fit_transform(X_train_v3, y_train_v3)\n",
    "    X_test_v3_selected = rfe.transform(X_test_v3)\n",
    "\n",
    "    gbc_model_v3 = GradientBoostingClassifier(random_state=42, n_estimators=5)\n",
    "    gbc_model_v3.fit(X_train_v3_selected, y_train_v3)\n",
    "\n",
    "    y_pred_v3 = gbc_model_v3.predict(X_test_v3_selected)\n",
    "\n",
    "    f1_gbc_v3_score = f1_score(y_test_v3, y_pred_v3, average='weighted')\n",
    "\n",
    "    rfe_f1_score_list.append(f1_gbc_v3_score)"
   ],
   "id": "4e9d7712b3270e8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = list(range(1, X_train_v3.shape[1]+1))\n",
    "y = rfe_f1_score_list\n",
    "\n",
    "ax.bar(x, y, width=0.2)\n",
    "ax.set_xlabel('Number of Selected Features (K)')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x, fontsize=12)\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index + 1, y=value + 0.02, s=f\"{value:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title('F1 Score vs Number of Selected Features (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "bea9015503aa225a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# at leash 3 features are needed to reach the baseline score\n",
    "rfe = RFE(estimator=GradientBoostingClassifier(random_state=42,\n",
    "                                               n_estimators=5),\n",
    "          n_features_to_select=3, # number of features to select\n",
    "          step=1,\n",
    "          verbose=0) # for large datasets, use step>1 to speed up the process\n",
    "\n",
    "X_train_v3_selected = rfe.fit_transform(X_train_v3, y_train_v3)\n",
    "selected_feature_mask = rfe.get_support()\n",
    "selected_features = X_train_v3.columns[selected_feature_mask]\n",
    "print('Most Important Features (K=3):')\n",
    "print(\"Selected Features:\", selected_features.tolist())"
   ],
   "id": "b512cf5343face53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Recursive Feature Elimination with Cross-Validation (RFECV) method\n",
    "- use an external model to assign weights to features and eliminate the least important ones recursively with cross-validation to find the optimal number of features"
   ],
   "id": "424d901a868f6145"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_v4, X_test_v4, y_train_v4, y_test_v4 = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ],
   "id": "1e7e5bb4f33d7dab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rfecv_f1_score_list = []\n",
    "\n",
    "for k in range(1, X_train_v4.shape[1]+1):\n",
    "\n",
    "    rfecv = RFECV(estimator=GradientBoostingClassifier(random_state=42,\n",
    "                                                       n_estimators=5),\n",
    "                  min_features_to_select=k, # minimum number of features to select\n",
    "                  step=1,\n",
    "                  cv=5, # number of folds in cross-validation\n",
    "                  scoring='accuracy',\n",
    "                  verbose=0) # for large datasets, use step>1 to speed up the process\n",
    "\n",
    "    X_train_v4_selected = rfecv.fit_transform(X_train_v4, y_train_v4)\n",
    "    X_test_v4_selected = rfecv.transform(X_test_v4)\n",
    "\n",
    "    gbc_model_v4 = GradientBoostingClassifier(random_state=42, n_estimators=5)\n",
    "    gbc_model_v4.fit(X_train_v4_selected, y_train_v4)\n",
    "\n",
    "    y_pred_v4 = gbc_model_v4.predict(X_test_v4_selected)\n",
    "\n",
    "    f1_gbc_v4_score = f1_score(y_test_v4, y_pred_v4, average='weighted')\n",
    "\n",
    "    rfecv_f1_score_list.append(f1_gbc_v4_score)"
   ],
   "id": "29c83ee24eb129a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the RFECV results\n",
    "fig, ax = plt.subplots()\n",
    "x = list(range(1, X_train_v4.shape[1]+1))\n",
    "y = rfecv_f1_score_list\n",
    "ax.bar(x, y, width=0.2)\n",
    "ax.set_xlabel('Number of Selected Features (K)')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x, fontsize=12)\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index + 1, y=value + 0.02, s=f\"{value:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title('F1 Score vs Number of Selected Features (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "edc251009af4c6ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# at leash 3 features are needed to reach the baseline score\n",
    "rfecv = RFECV(estimator=GradientBoostingClassifier(random_state=42,\n",
    "                                                   n_estimators=5),\n",
    "              min_features_to_select=3, # minimum number of features to select\n",
    "              step=1,\n",
    "              cv=5, # number of folds in cross-validation\n",
    "              scoring='accuracy',\n",
    "              verbose=0) # for large datasets, use step>1 to speed up the process\n",
    "\n",
    "X_train_v4_selected = rfecv.fit_transform(X_train_v4, y_train_v4)\n",
    "selected_feature_mask = rfecv.get_support()\n",
    "selected_features = X_train_v4.columns[selected_feature_mask]\n",
    "\n",
    "print('Most Important Features (K=3):')\n",
    "print(\"Selected Features:\", selected_features.tolist())"
   ],
   "id": "aa931c5c0f85f00c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Boruta method\n",
    "- use a tree-based model to assign importance to features and select the most important ones by comparing them to their shadow features"
   ],
   "id": "9ce195df9782f78c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_v5, X_test_v5, y_train_v5, y_test_v5 = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ],
   "id": "2b663f4c7c4c08f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gbc = GradientBoostingClassifier(random_state=42, n_estimators=5)\n",
    "boruta_selector = BorutaPy(estimator=gbc,\n",
    "                             random_state=42)\n",
    "\n",
    "boruta_selector.fit(X_train_v5.values, y_train_v5.values) # quite a slow process 3m 46s\n",
    "\n",
    "sel_X_train_v5 = boruta_selector.transform(X_train_v5.values)\n",
    "sel_X_test_v5 = boruta_selector.transform(X_test_v5.values)\n",
    "\n",
    "gbc.fit(sel_X_train_v5, y_train_v5)\n",
    "\n",
    "y_pred_v5 = gbc.predict(sel_X_test_v5)\n",
    "f1_gbc_v5_score = f1_score(y_test_v5, y_pred_v5, average='weighted')\n",
    "\n",
    "print(f\"F1 Score with Boruta: {f1_gbc_v5_score:.4f}\")"
   ],
   "id": "723e6b2ccc02da6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "selected_features = X_train_v5.columns[boruta_selector.support_].to_list()\n",
    "print('Most Important Features (Boruta):')\n",
    "print(\"Selected Features:\", selected_features)"
   ],
   "id": "ea7a2be663f7de42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare all feature selection methods using bar plot",
   "id": "15b8278e515d53fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = ['All Features', 'Variance Threshold', 'K-Best', 'RFE', 'RFECV', 'Boruta']\n",
    "y = [f1_gbc_score, f1_gbc_v1_score, max(f1_score_list), max(rfe_f1_score_list), max(rfecv_f1_score_list), f1_gbc_v5_score]\n",
    "\n",
    "ax.bar(x, y, width=0.2, color=['blue', 'orange', 'green', 'red', 'purple', 'brown'])\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_xlabel('Feature Selection Method')\n",
    "ax.set_ylim(0, 1.2)\n",
    "\n",
    "for index, value in enumerate(y):\n",
    "    ax.text(x=index, y=value + 0.02, s=f\"{value:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "plt.title('F1 Score Comparison of Feature Selection Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "11b996b715fdc900",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
