{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Spam Classification Problem\n",
    "\n",
    "In this notebook, we will use the Naive Bayes algorithm to build a spam filter that classifies SMS messages as spam or non-spam.\n",
    "\n",
    " To train the algorithm, we will use a dataset of 5,572 SMS messages that are already classified by humans."
   ],
   "id": "6e43bf341f550801"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Main goal: Build ANN model to recognize spam messages.\n",
    "\n",
    "> Feature Variables\n",
    "> - SMS message\n",
    "\n",
    "> Target class: Message type\n",
    "> - Spam or Ham\n"
   ],
   "id": "c64fd7f6ae047bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n"
   ],
   "id": "c896b7565d8aaf03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Input preprocessing\n",
    "\n",
    "# 1. Cleansing\n",
    "# 2. Stop word removal\n",
    "# 3. Lemmatization\n",
    "# 4. Numeric representation ->  TF-IDF vs word embeddings \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "id": "3417bf3b6387f2c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Load Spam Data and review content\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "\n",
    "\n",
    "#Separate feature and target data\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]"
   ],
   "id": "1f88833aad41e76c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Custom tokenizer to remove stopwords and use lemmatization\n",
    "def custom_tokenize(string):\n",
    "    #Split string as tokens\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    #Filter for stopwords\n",
    "    nostop = list(filter(lambda token: token not in stopwords.words('english'), tokens))\n",
    "    #Perform lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in nostop ]\n",
    "    return lemmatized"
   ],
   "id": "1161eaf2f134d9ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import utils as k_utils\n",
    "\n",
    "#Build a TF-IDF Vectorizer model\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenize)\n",
    "\n",
    "#Transform feature input to TF-IDF\n",
    "tfidf=vectorizer.fit_transform(spam_messages)\n",
    "#Convert TF-IDF to numpy array\n",
    "tfidf_array = tfidf.toarray()\n",
    "\n",
    "#Build a label encoder for target variable to convert strings to numeric values.\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "spam_classes = label_encoder.fit_transform(spam_classes_raw)\n",
    "\n",
    "#Convert target to one-hot encoding vector\n",
    "spam_classes = k_utils.to_categorical(spam_classes,2)\n",
    "\n",
    "print(\"TF-IDF Matrix Shape : \", tfidf.shape)\n",
    "print(\"One-hot Encoding Shape : \", spam_classes.shape)"
   ],
   "id": "e095b667609c9243"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# split data into training and test sets\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(tfidf_array, spam_classes, test_size=0.10)\n"
   ],
   "id": "a3a620931e15e47f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Building the ANN Model\n",
    "\n",
    "NB_CLASSES=2\n",
    "N_HIDDEN=32\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                             input_shape=(X_train.shape[1],),\n",
    "                              name='Hidden-Layer-1',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "                              name='Hidden-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "id": "3c035be41219fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training the Model\n",
    "\n",
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=10\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nAccuracy during Training :\\n------------------------------------\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history)[\"accuracy\"].plot(figsize=(8, 5))\n",
    "plt.title(\"Accuracy improvements with Epoch\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ],
   "id": "5bb6db14cf8fdcb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model Evaluation (predict for text)\n",
    "\n",
    "#Predict for multiple samples using batch processing\n",
    "\n",
    "#Convert input into IF-IDF vector using the same vectorizer model\n",
    "predict_tfidf=vectorizer.transform([\"FREE entry to a fun contest\",\n",
    "                                    \"Yup I will come over\"]).toarray()\n",
    "\n",
    "print(predict_tfidf.shape)\n",
    "\n",
    "#Predict using model\n",
    "prediction=np.argmax( model.predict(predict_tfidf), axis=1 )\n",
    "print(\"Prediction Output:\" , prediction)\n",
    "\n",
    "#Print prediction classes\n",
    "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
   ],
   "id": "19cd334dbbddeda2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
